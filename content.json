{"meta":{"title":"珍品菲尔的博客","subtitle":"Lifetime Learner","description":"时间顺流而下，生活逆水行舟","author":"cuanHaoQi","url":"http://example.com","root":"/"},"pages":[{"title":"标签","date":"2021-10-12T08:17:54.000Z","updated":"2021-10-12T08:18:42.419Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"数据开发面试题","slug":"数据开发面试题","date":"2022-04-24T06:32:34.000Z","updated":"2022-04-24T07:03:08.520Z","comments":true,"path":"2022/04/24/数据开发面试题/","link":"","permalink":"http://example.com/2022/04/24/%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%A2%98/","excerpt":"HadoopHadoop基础先说下Hadoop是什么Hadoop是一个分布式系统基础架构，主要是为了解决海量数据的存储和海量数据的分析计算问题。 说下Hadoop核心组件Hadoop自诞生以来，主要有Hadoop 1.x、2.x、3.x三个系列多个版本； Hadoop 1.x组成：HDFS（具有高可靠性、高吞吐量的分布式文件系统，用于数据存储），MapReduce（同时处理业务逻辑运算和资源的调度），Common（辅助工具，为其它Hadoop模块提供基础设施）； Hadoop 2.x和Hadoop 3.x组成上无变化，和Hadoop 1.x相比，增加了YARN，分担了MapReduce的工作，组件包括：HDFS（具有高可靠性、高吞吐量的分布式文件系统，用于数据存储），MapReduce（处理业务逻辑运算），YARN（负责作业调度与集群资源管理），Common（辅助工具，为其它Hadoop模块提供基础设施）。","text":"HadoopHadoop基础先说下Hadoop是什么Hadoop是一个分布式系统基础架构，主要是为了解决海量数据的存储和海量数据的分析计算问题。 说下Hadoop核心组件Hadoop自诞生以来，主要有Hadoop 1.x、2.x、3.x三个系列多个版本； Hadoop 1.x组成：HDFS（具有高可靠性、高吞吐量的分布式文件系统，用于数据存储），MapReduce（同时处理业务逻辑运算和资源的调度），Common（辅助工具，为其它Hadoop模块提供基础设施）； Hadoop 2.x和Hadoop 3.x组成上无变化，和Hadoop 1.x相比，增加了YARN，分担了MapReduce的工作，组件包括：HDFS（具有高可靠性、高吞吐量的分布式文件系统，用于数据存储），MapReduce（处理业务逻辑运算），YARN（负责作业调度与集群资源管理），Common（辅助工具，为其它Hadoop模块提供基础设施）。 Hadoop核心组件作用 Hadoop主要组件如上图，主要是HDFS、MapReduce、YARN、Common HDFS HDFS是一个文件系统，用于存储文件，通过目录树来定位文件。 其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 HDFS 的使用场景：适合一次写入，多次读出的场景。一个文件经过创建、写入和关闭之后就不需要改变。 MapReduce MapReduce是一个分布式运算程序的编程框架，是用户开发“基于Hadoop的数据分析应用”的核心框架。 MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。 MapReduce将计算过程分为两个阶段：Map和Reduce Map阶段并行处理输入数据 Reduce阶段对Map结果进行汇总 YARN 先来看两个问题，在Hadoop中 如何管理集群资源？ 如何给任务合理分配资源？ YARN在Hadoop中的作用，就是上面两个问题的答案。Yarn 是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 Common Hadoop体系最底层的一个模块，为Hadoop各子项目提供各种工具，如：配置文件和日志操作等。 集群的最主要瓶颈&emsp; 磁盘IO Hadoop运行模式&emsp; 单机版、伪分布式模式、完全分布式模式 Hadoop生态圈的组件并做简要描述&emsp; 1）Zookeeper：是一个开源的分布式应用程序协调服务,基于zookeeper可以实现同步服务，配置维护，命名服务。&emsp; 2）Flume：一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。&emsp; 3）Hbase：是一个分布式的、面向列的开源数据库, 利用Hadoop HDFS作为其存储系统。&emsp; 4）Hive：基于Hadoop的一个数据仓库工具，可以将结构化的数据档映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。&emsp; 5）Sqoop：将一个关系型数据库中的数据导进到Hadoop的 HDFS中，也可以将HDFS的数据导进到关系型数据库中。 解释“hadoop”和“hadoop 生态系统”两个概念&emsp; Hadoop是指Hadoop框架本身；hadoop生态系统，不仅包含hadoop，还包括保证hadoop框架正常高效运行其他框架，比如zookeeper、Flume、Hbase、Hive、Sqoop等辅助框架。 请列出正常工作的Hadoop集群中Hadoop都分别需要启动哪些进程，它们的作用分别是什么?&emsp; 1）NameNode：它是hadoop中的主服务器，管理文件系统名称空间和对集群中存储的文件的访问，保存有metadate。&emsp; 2）SecondaryNameNode：它不是namenode的冗余守护进程，而是提供周期检查点和清理任务。帮助NN合并editslog，减少NN启动时间。&emsp; 3）DataNode：它负责管理连接到节点的存储（一个集群中可以有多个节点）。每个存储数据的节点运行一个datanode守护进程。&emsp; 4）ResourceManager（JobTracker）：JobTracker负责调度DataNode上的工作。每个DataNode有一个TaskTracker，它们执行实际工作。&emsp; 5）NodeManager：（TaskTracker）执行任务。&emsp; 6）DFSZKFailoverController：高可用时它负责监控NN的状态，并及时的把状态信息写入ZK。它通过一个独立线程周期性的调用NN上的一个特定接口来获取NN的健康状态。FC也有选择谁作为Active NN的权利，因为最多只有两个节点，目前选择策略还比较简单（先到先得，轮换）。&emsp; 7）JournalNode：高可用情况下存放namenode的editlog文件。 HDFSHDFS 中的 block 默认保存几份？&emsp; 默认保存3份 HDFS 默认 BlockSize 是多大？&emsp; Hadoop1.x 默认64MB, Hadoop 2.x 3.x默认128MB 负责HDFS数据存储的是哪一部分？&emsp; DataNode负责数据存储 SecondaryNameNode的目的是什么？&emsp; 他的目的使帮助NameNode合并编辑日志，减少NameNode 启动时间 文件大小设置，增大有什么影响？&emsp; HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。&emsp; 思考：为什么块的大小不能设置的太小，也不能设置的太大？&emsp; &emsp; HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。&emsp; 如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB。默认的块大小128MB。&emsp; 块的大小：10ms×100×100M/s = 100M，如图 &emsp; 增加文件块大小，需要增加磁盘的传输速率。 hadoop的块大小，从哪个版本开始是128M&emsp; Hadoop1.x都是64M，hadoop2.x开始都是128M。 HDFS的存储机制（☆☆☆☆☆）&emsp; HDFS存储机制，包括HDFS的写入数据过程和读取数据过程两部分&emsp; HDFS写数据过程 &emsp; 1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。&emsp; 2）NameNode返回是否可以上传。&emsp; 3）客户端请求第一个 block上传到哪几个datanode服务器上。&emsp; 4）NameNode返回3个datanode节点，分别为dn1、dn2、dn3。&emsp; 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。&emsp; 6）dn1、dn2、dn3逐级应答客户端。&emsp; 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。&emsp; 8）当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步）。 &emsp; HDFS读数据过程 &emsp; 1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。&emsp; 2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。&emsp; 3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以packet为单位来做校验）。&emsp; 4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。 secondary namenode工作机制（☆☆☆☆☆） 1）第一阶段：NameNode启动&emsp; （1）第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。&emsp; （2）客户端对元数据进行增删改的请求。&emsp; （3）NameNode记录操作日志，更新滚动日志。&emsp; （4）NameNode在内存中对数据进行增删改查。2）第二阶段：Secondary NameNode工作&emsp; （1）Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。&emsp; （2）Secondary NameNode请求执行checkpoint。&emsp; （3）NameNode滚动正在写的edits日志。&emsp; （4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。&emsp; （5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。&emsp; （6）生成新的镜像文件fsimage.chkpoint。&emsp; （7）拷贝fsimage.chkpoint到NameNode。&emsp; （8）NameNode将fsimage.chkpoint重新命名成fsimage。 NameNode与SecondaryNameNode 的区别与联系？（☆☆☆☆☆）机制流程看第7题1）区别&emsp; （1）NameNode负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。&emsp; （2）SecondaryNameNode主要用于定期合并命名空间镜像和命名空间镜像的编辑日志。2）联系：&emsp; （1）SecondaryNameNode中保存了一份和namenode一致的镜像文件（fsimage）和编辑日志（edits）。&emsp; （2）在主namenode发生故障时（假设没有及时备份数据），可以从SecondaryNameNode恢复数据。 HDFS组成架构（☆☆☆☆☆） 架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面我们分别介绍这四个组成部分。1）Client：就是客户端。&emsp; （1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储；&emsp; （2）与NameNode交互，获取文件的位置信息；&emsp; （3）与DataNode交互，读取或者写入数据；&emsp; （4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS；&emsp; （5）Client可以通过一些命令来访问HDFS；2）NameNode：就是Master，它是一个主管、管理者。&emsp; （1）管理HDFS的名称空间；&emsp; （2）管理数据块（Block）映射信息；&emsp; （3）配置副本策略；&emsp; （4）处理客户端读写请求。3）DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。&emsp; （1）存储实际的数据块；&emsp; （2）执行数据块的读/写操作。4）Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。&emsp; （1）辅助NameNode，分担其工作量；&emsp; （2）定期合并Fsimage和Edits，并推送给NameNode；&emsp; （3）在紧急情况下，可辅助恢复NameNode。 HAnamenode 是如何工作的? （☆☆☆☆☆） ZKFailoverController主要职责&emsp; 1）健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态。&emsp; 2）会话管理：如果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主NN，同时标记状态为Active。&emsp; 3）当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN。&emsp; 4）master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态 HDFS读写数据流程详解先来看下机架感知机制，也就是HDFS上副本存储结点的选择。 Hadoop3.x副本结点选择： 由上图可知，第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。 第二个副本在另一个机架的随机一个节点。 第三个副本在第二个副本所在机架的随机节点。 关于HDFS读写流程，这里还是给出两个版本，有助于理解 第一个版本：简洁版HDFS写数据流程 1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 2）NameNode返回是否可以上传。 3）客户端请求第一个 block上传到哪几个datanode服务器上。 4）NameNode返回3个datanode节点，分别为dn1、dn2、dn3。 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。 7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 8）当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步）。 HDFS读数据流程 1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。 2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。 3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以packet为单位来做校验）。 4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。 第二个版本：详细版，有助于理解HDFS写数据流程 1）Client将File按128M分块。分成两块，Block1和Block2; 2）Client向nameNode发送写数据请求，如图蓝色虚线①———&gt;。 3）NameNode节点，记录block信息。并返回可用的DataNode，如粉色虚线②———-&gt;。 Block1: host2,host3,host1 Block2: host7,host8,host4 4）client向DataNode发送block1；发送过程是以流式写入。 流式写入过程： （1）将64M的block1按64k的package划分; （2）然后将第一个package发送给host2; （3）host2接收完后，将第一个package发送给host3，同时client向host2发送第二个package； （4）host3接收完第一个package后，发送给host1，同时接收host2发来的第二个package。 （5）以此类推，如图红线实线所示，直到将block1发送完毕。 （6）host2，host3，host1向NameNode，host2向Client发送通知，说“消息发送完了”。如图粉红颜色实线所示。 （7）client收到host2发来的消息后，向namenode发送消息，说我写完了。这样就完成了。如图黄色粗实线。 （8）发送完block1后，再向host7，host8，host4发送block2，如图蓝色实线所示。 （9）发送完block2后，host7，host8，host4向NameNode，host7向Client发送通知，如图浅绿色实线所示。 （10）client向NameNode发送消息，说我写完了，如图黄色粗实线。。。这样就完毕了。 HDFS读数据流程 1）client向namenode发送读请求。 2）namenode查看Metadata信息，返回fileA的block的位置。 Block1: host2,host3,host1 Block2: host7,host8,host4 3）block的位置是有先后顺序的，先读block1，再读block2。而且block1去host2上读取；然后block2，去host7上读取。 MapReduce谈谈Hadoop序列化和反序列化及自定义bean对象实现序列化?1）序列化和反序列化&emsp; （1）序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。&emsp; （2）反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。&emsp; （3）Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简、高效。2）自定义bean对象要想序列化传输步骤及注意事项：&emsp; （1）必须实现Writable接口&emsp; （2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造&emsp; （3）重写序列化方法&emsp; （4）重写反序列化方法&emsp; （5）注意反序列化的顺序和序列化的顺序完全一致&emsp; （6）要想把结果显示在文件中，需要重写toString()，且用”\\t”分开，方便后续用&emsp; （7）如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序 FileInputFormat切片机制（☆☆☆☆☆）job提交流程源码详解&emsp; waitForCompletion()&emsp; submit();&emsp; // 1、建立连接&emsp; &emsp; connect();&emsp; &emsp; &emsp; // 1）创建提交job的代理&emsp; &emsp; &emsp; new Cluster(getConfiguration());&emsp; &emsp; &emsp; &emsp; // （1）判断是本地yarn还是远程&emsp; &emsp; &emsp; &emsp; initialize(jobTrackAddr, conf);&emsp; // 2、提交job&emsp; submitter.submitJobInternal(Job.this, cluster)&emsp; &emsp; // 1）创建给集群提交数据的Stag路径&emsp; &emsp; Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);&emsp; &emsp; // 2）获取jobid ，并创建job路径&emsp; &emsp; JobID jobId = submitClient.getNewJobID();&emsp; &emsp; // 3）拷贝jar包到集群&emsp; &emsp; copyAndConfigureFiles(job, submitJobDir);&emsp; &emsp; rUploader.uploadFiles(job, jobSubmitDir);&emsp; &emsp; // 4）计算切片，生成切片规划文件&emsp; &emsp; writeSplits(job, submitJobDir);&emsp; &emsp; maps = writeNewSplits(job, jobSubmitDir);&emsp; &emsp; input.getSplits(job);&emsp; &emsp; // 5）向Stag路径写xml配置文件&emsp; &emsp; writeConf(conf, submitJobFile);&emsp; &emsp; conf.writeXml(out);&emsp; &emsp; // 6）提交job,返回提交状态&emsp; &emsp; status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); 在一个运行的Hadoop 任务中，什么是InputSplit？（☆☆☆☆☆）FileInputFormat源码解析(input.getSplits(job))（1）找到你数据存储的目录。（2）开始遍历处理（规划切片）目录下的每一个文件。（3）遍历第一个文件ss.txt。&emsp; a）获取文件大小fs.sizeOf(ss.txt);。&emsp; b）计算切片大小computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M。&emsp; c）默认情况下，切片大小=blocksize。&emsp; d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）。&emsp; e）将切片信息写到一个切片规划文件中。&emsp; f）整个切片的核心过程在getSplit()方法中完成。&emsp; g）数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。&emsp; h）注意：block是HDFS上物理上存储的存储的数据，切片是对数据逻辑上的划分。（4）提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。 如何判定一个job的map和reduce的数量?1）map数量&emsp; splitSize=max{minSize,min{maxSize,blockSize}}&emsp; map数量由处理的数据分成的block数量决定default_num = total_size / split_size;2）reduce数量&emsp; reduce的数量job.setNumReduceTasks(x);x 为reduce的数量。不设置的话默认为 1。 Maptask的个数由什么决定？&emsp; 一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定。 MapTask和ReduceTask工作机制（☆☆☆☆☆）（也可回答MapReduce工作原理）MapTask工作机制 （1）Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。（2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。（4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。（5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 ReduceTask工作机制 （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。（2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。（3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。 由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。（4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 描述mapReduce有几种排序及排序发生的阶段（☆☆☆☆☆）1）排序的分类：&emsp; （1）部分排序：&emsp; &emsp; MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。&emsp; （2）全排序：&emsp; &emsp; 如何用Hadoop产生一个全局排序的文件？最简单的方法是使用一个分区。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。&emsp; &emsp; 替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为待分析文件创建3个分区，在第一分区中，记录的单词首字母a-g，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。&emsp; （3）辅助排序：（GroupingComparator分组）&emsp; &emsp; Mapreduce框架在记录到达reducer之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。&emsp; （4）二次排序：&emsp; &emsp; 在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。2）自定义排序WritableComparable&emsp; bean对象实现WritableComparable接口重写compareTo方法，就可以实现排序&emsp; &emsp; @Override&emsp; &emsp; public int compareTo(FlowBean o) {&emsp; &emsp; &emsp; // 倒序排列，从大到小&emsp; &emsp; &emsp; return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;&emsp; &emsp; }3）排序发生的阶段：&emsp; （1）一个是在map side发生在spill后partition前。&emsp; （2）一个是在reduce side发生在copy后 reduce前。 描述mapReduce中shuffle阶段的工作流程，如何优化shuffle阶段（☆☆☆☆☆） 分区，排序，溢写，拷贝到对应reduce机器上，增加combiner，压缩溢写的文件。 描述mapReduce中combiner的作用是什么，一般使用情景，哪些情况不需要，及和reduce的区别？1）Combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。2）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟reducer的输入kv类型要对应起来。3）Combiner和reducer的区别在于运行的位置。&emsp; Combiner是在每一个maptask所在的节点运行；&emsp; Reducer是接收全局所有Mapper的输出结果。 如果没有定义partitioner，那数据在被送达reducer前是如何被分区的？&emsp; 如果没有自定义的 partitioning，则默认的 partition 算法，即根据每一条数据的 key 的 hashcode 值摸运算（%）reduce 的数量，得到的数字就是“分区号“。 MapReduce 出现单点负载多大，怎么负载平衡？ （☆☆☆☆☆）&emsp; 通过Partitioner实现 MapReduce 怎么实现 TopN？ （☆☆☆☆☆）&emsp; 可以自定义groupingcomparator，对结果进行最大值排序，然后再reduce输出时，控制只输出前n个数。就达到了topn输出的目的。 Hadoop的缓存机制（Distributedcache）（☆☆☆☆☆）&emsp; 分布式缓存一个最重要的应用就是在进行join操作的时候，如果一个表很大，另一个表很小，我们就可以将这个小表进行广播处理，即每个计算节点上都存一份，然后进行map端的连接操作，经过我的实验验证，这种情况下处理效率大大高于一般的reduce端join，广播处理就运用到了分布式缓存的技术。&emsp; DistributedCache将拷贝缓存的文件到Slave节点在任何Job在节点上执行之前，文件在每个Job中只会被拷贝一次，缓存的归档文件会被在Slave节点中解压缩。将本地文件复制到HDFS中去，接着Client会通过addCacheFile() 和addCacheArchive()方法告诉DistributedCache在HDFS中的位置。当文件存放到文地时，JobClient同样获得DistributedCache来创建符号链接，其形式为文件的URI加fragment标识。当用户需要获得缓存中所有有效文件的列表时，JobConf 的方法 getLocalCacheFiles() 和getLocalArchives()都返回一个指向本地文件路径对象数组。 如何使用mapReduce实现两个表的join?（☆☆☆☆☆）&emsp; 1）reduce side join : 在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签（tag）,比如：tag=0 表示来自文件File1，tag=2 表示来自文件File2。&emsp; 2）map side join : Map side join 是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多份，让每个map task 内存中存在一份（比如存放到hash table 中），然后只扫描大表：对于大表中的每一条记录key/value，在hash table 中查找是否有相同的key 的记录，如果有，则连接后输出即可。 什么样的计算不能用mr来提速？&emsp; 1）数据量很小。&emsp; 2）繁杂的小文件。&emsp; 3）索引是更好的存取机制的时候。&emsp; 4）事务处理。&emsp; 5）只有一台机器的时候。 ETL是哪三个单词的缩写&emsp; Extraction-Transformation-Loading的缩写，中文名称为数据提取、转换和加载。 介绍下MapReduce可以结合MapReduce的优缺点一起回答 MapReduce 是一个分布式运算程序的编程框架，它的核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个 Hadoop 集群上。 MapReduce的核心思想是将用户编写的逻辑代码和架构中的各个组件整合成一个分布式运算程序，实现一定程序的并行处理海量数据，提高效率。 海量数据难以在单机上处理，而一旦将单机版程序扩展到集群上进行分布式运行势必将大大增加程序的复杂程度。引入MapReduce架构，开发人员可以将精力集中于数据处理的核心业务逻辑上，而将分布式程序中的公共功能封装成框架,以降低开发的难度。 一个完整的mapreduce程序有三类实例进程 MRAppMaster：负责整个程序的协调过程 MapTask：负责map阶段的数据处理 ReduceTask：负责reduce阶段的数据处理 MapReduce优缺点优点1）MapReduce 易于编程 它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得 MapReduce 编程变得非常流行。 2）良好的扩展性 当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 3）高容错性 MapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行， 不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由 Hadoop 内部完成的。 4）适合 PB 级以上海量数据的离线处理 可以实现上千台服务器集群并发工作，提供数据处理能力。 缺点1）不擅长实时计算 MapReduce无法像MySQL一样，在毫秒或者秒级内返回结果。 2）不擅长流式计算 流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。 3）不擅长 DAG（有向无环图）计算 多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘， 会造成大量的磁盘 IO，导致性能非常的低下。 Yarn简述hadoop1与hadoop2 的架构异同&emsp; 1）加入了yarn解决了资源调度的问题。&emsp; 2）加入了对zookeeper的支持实现比较可靠的高可用。 为什么会产生 yarn,它解决了什么问题，有什么优势？&emsp; 1）Yarn最主要的功能就是解决运行的用户程序与yarn框架完全解耦。&emsp; 2）Yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce、storm程序，spark程序…… HDFS的数据压缩算法?（☆☆☆☆☆）&emsp; Hadoop中常用的压缩算法有bzip2、gzip、lzo、snappy，其中lzo、snappy需要操作系统安装native库才可以支持。&emsp; 数据可以压缩的位置如下所示。 &emsp; 企业开发用的比较多的是snappy。 Hadoop的调度器总结（☆☆☆☆☆）（1）默认的调度器FIFO&emsp; Hadoop中默认的调度器，它先按照作业的优先级高低，再按照到达时间的先后选择被执行的作业。（2）计算能力调度器Capacity Scheduler&emsp; 支持多个队列，每个队列可配置一定的资源量，每个队列采用FIFO调度策略，为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。调度时，首先按以下策略选择一个合适队列：计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值，选择一个该比值最小的队列；然后按以下策略选择该队列中一个作业：按照作业优先级和提交时间顺序选择，同时考虑用户资源量限制和内存限制。（3）公平调度器Fair Scheduler&emsp; 同计算能力调度器类似，支持多队列多用户，每个队列中的资源量可以配置，同一队列中的作业公平共享队列中所有资源。实际上，Hadoop的调度器远不止以上三种，最近，出现了很多针对新型应用的Hadoop调度器。 MapReduce 2.0 容错性（☆☆☆☆☆）1）MRAppMaster容错性&emsp; 一旦运行失败，由YARN的ResourceManager负责重新启动，最多重启次数可由用户设置，默认是2次。一旦超过最高重启次数，则作业运行失败。2）Map Task/Reduce&emsp; Task Task周期性向MRAppMaster汇报心跳；一旦Task挂掉，则MRAppMaster将为之重新申请资源，并运行之。最多重新运行次数可由用户设置，默认4次。 mapreduce推测执行算法及原理（☆☆☆☆☆）1）作业完成时间取决于最慢的任务完成时间&emsp; 一个作业由若干个Map 任务和Reduce 任务构成。因硬件老化、软件Bug 等，某些任务可能运行非常慢。&emsp; 典型案例：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？2）推测执行机制&emsp; 发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。3）不能启用推测执行机制情况&emsp; （1）任务间存在严重的负载倾斜；&emsp; （2）特殊任务，比如任务向数据库中写数据。4）算法原理&emsp; 假设某一时刻，任务T的执行进度为progress，则可通过一定的算法推测出该任务的最终完成时刻estimateEndTime。另一方面，如果此刻为该任务启动一个备份任务，则可推断出它可能的完成时刻estimateEndTime,于是可得出以下几个公式： 123estimateEndTime=estimatedRunTime+taskStartTime estimatedRunTime=(currentTimestamp-taskStartTime)/progress estimateEndTime`= currentTimestamp+averageRunTime &emsp; 其中，currentTimestamp为当前时刻；taskStartTime为该任务的启动时刻；averageRunTime为已经成功运行完成的任务的平均运行时间。这样，MRv2总是选择（estimateEndTime- estimateEndTime·）差值最大的任务，并为之启动备份任务。为了防止大量任务同时启动备份任务造成的资源浪费，MRv2为每个作业设置了同时启动的备份任务数目上限。&emsp; 推测执行机制实际上采用了经典的算法优化方法：以空间换时间，它同时启动多个相同任务处理相同的数据，并让这些任务竞争以缩短数据处理时间。显然，这种方法需要占用更多的计算资源。在集群资源紧缺的情况下，应合理使用该机制，争取在多用少量资源的情况下，减少作业的计算时间。 介绍下YARN介绍YARN，可以先考虑下面两个问题 1）如何管理集群资源？ 2）如何给任务合理分配资源？ YARN是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 YARN 作为一个资源管理、任务调度的框架，主要包含ResourceManager、NodeManager、ApplicationMaster和Container模块。 YARN基础架构 1）ResourceManager（RM）主要作用如下： 处理客户端请求 监控NodeManager 启动或监控ApplicationMaster 资源的分配与调度 2）NodeManager（NM）主要作用如下： 管理单个节点上的资源 处理来自ResourceManager的命令 处理来自ApplicationMaster的命令 3）ApplicationMaster（AM）作用如下： 为应用程序申请资源并分配给内部的任务 任务的监督与容错 4）Container Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。 可以结合“YARN有什么优势，能解决什么问题？”一起回答 优化问题MapReduce跑得慢的原因？（☆☆☆☆☆）Mapreduce 程序效率的瓶颈在于两点：1）计算机性能&emsp; CPU、内存、磁盘健康、网络2）I/O 操作优化&emsp; （1）数据倾斜&emsp; （2）map和reduce数设置不合理&emsp; （3）reduce等待过久&emsp; （4）小文件过多&emsp; （5）大量的不可分块的超大文件&emsp; （6）spill次数过多&emsp; （7）merge次数过多等 MapReduce优化方法（☆☆☆☆☆）1）数据输入&emsp; （1）合并小文件：在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致mr运行较慢。&emsp; （2）采用ConbinFileInputFormat来作为输入，解决输入端大量小文件场景。2）map阶段&emsp; （1）减少spill次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘 IO。&emsp; （2）减少merge次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。&emsp; （3）在 map 之后先进行combine处理，减少I/O。3）reduce阶段&emsp; （1）合理设置map和reduce数：两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致 map、reduce任务间竞争资源，造成处理超时等错误。&emsp; （2）设置map、reduce共存：调整slowstart.completedmaps参数，使map运行到一定程度后，reduce也开始运行，减少reduce的等待时间。&emsp; （3）规避使用reduce，因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。&emsp; （4）合理设置reduce端的buffer，默认情况下，数据达到一个阈值的时候，buffer中的数据就会写入磁盘，然后reduce会从磁盘中获得所有的数据。也就是说，buffer和reduce是没有直接关联的，中间多个一个写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得buffer中的一部分数据可以直接输送到reduce，从而减少IO开销：mapred.job.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读buffer中的数据直接拿给reduce使用。这样一来，设置buffer需要内存，读取数据需要内存，reduce计算也要内存，所以要根据作业的运行情况进行调整。4）IO传输&emsp; （1）采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZOP压缩编码器。&emsp; （2）使用SequenceFile二进制文件5）数据倾斜问题&emsp; （1）数据倾斜现象&emsp; &emsp; 数据频率倾斜——某一个区域的数据量要远远大于其他区域。&emsp; &emsp; 数据大小倾斜——部分记录的大小远远大于平均值。&emsp; （2）如何收集倾斜数据&emsp; &emsp; 在reduce方法中加入记录map输出键的详细情况的功能。 123456789101112131415161718192021public static final String MAX_VALUES = &quot;skew.maxvalues&quot;;private int maxValueThreshold;@Overridepublic void configure(JobConf job) &#123; maxValueThreshold = job.getInt(MAX_VALUES, 100);&#125;@Overridepublic void reduce(Text key, Iterator&lt;Text&gt; values, OutputCollector&lt;Text, Text&gt; output, Reporter reporter) throws IOException &#123; int i = 0; while (values.hasNext()) &#123; values.next(); i++; &#125; if (++i &gt; maxValueThreshold) &#123; log.info(&quot;Received &quot; + i + &quot; values for key &quot; + key); &#125;&#125; &emsp; （3）减少数据倾斜的方法&emsp; &emsp; 方法1：抽样和范围分区&emsp; &emsp; &emsp; 可以通过对原始数据进行抽样得到的结果集来预设分区边界值。&emsp; &emsp; 方法2：自定义分区&emsp; &emsp; &emsp; 另一个抽样和范围分区的替代方案是基于输出键的背景知识进行自定义分区。例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。&emsp; &emsp; 方法3：Combine&emsp; &emsp; &emsp; 使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。在可能的情况下，combine的目的就是聚合并精简数据。 HDFS小文件优化方法（☆☆☆☆☆）1）HDFS小文件弊端：&emsp; HDFS上每个文件都要在namenode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用namenode的内存空间，另一方面就是索引文件过大是的索引速度变慢。2）解决的方式：&emsp; （1）Hadoop本身提供了一些文件压缩的方案。&emsp; （2）从系统层面改变现有HDFS存在的问题，其实主要还是小文件的合并，然后建立比较快速的索引。3）Hadoop自带小文件解决方案&emsp; （1）Hadoop Archive：&emsp; &emsp; 是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时。&emsp; （2）Sequence file：&emsp; &emsp; sequence file由一系列的二进制key/value组成，如果为key小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。&emsp; （3）CombineFileInputFormat：&emsp; &emsp; CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。 HiveHive概述什么是Hive&emsp; Hive是由Facebook开源用于解决海量结构化日志的数据统计。&emsp; Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。&emsp; 本质是：将HQL转化成MapReduce程序 &emsp; 1）Hive处理的数据存储在HDFS&emsp; 2）Hive分析数据底层的实现是MapReduce&emsp; 3）执行程序运行在Yarn上 Hive是Hadoop生态系统中比不可少的一个工具，它提供了一种SQL(结构化查询语言)方言，可以查询存储在Hadoop分布式文件系统（HDFS）中的数据或其他和Hadoop集成的文件系统，如MapR-FS、Amazon的S3和像HBase（Hadoop数据仓库）和Cassandra这样的数据库中的数据。 大多数数据仓库应用程序都是使用关系数据库进行实现的，并使用SQL作为查询语言。Hive降低了将这些应用程序转移到Hadoop系统上的难度。凡是会使用SQL语言的开发人员都可以很轻松的学习并使用Hive。如果没有Hive，那么这些用户就必须学习新的语言和工具，然后才能应用到生产环境中。另外，相比其他工具，Hive更便于开发人员将基于SQL的应用程序转移到Hadoop中。如果没有Hive，那么开发者将面临一个艰巨的挑战，如何将他们的SQL应用程序移植到Hadoop上。 Hive优缺点优点：&emsp; 1) 操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。&emsp; 2) 避免了去写MapReduce，减少开发人员的学习成本。&emsp; 3) Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。&emsp; 4) Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。&emsp; 5) Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。缺点：&emsp; 1）Hive的HQL表达能力有限&emsp; &emsp; （1）迭代式算法无法表达&emsp; &emsp; （2）数据挖掘方面不擅长&emsp; 2）Hive的效率比较低&emsp; &emsp; （1）Hive自动生成的MapReduce作业，通常情况下不够智能化&emsp; &emsp; （2）Hive调优比较困难，粒度较粗 Hive不是一个完整的数据库。Hadoop以及HDFS的设计本身约束和局限性地限制了Hive所能胜任的工作。其中最大的限制就是Hive不支持记录级别的更新、插入或者删除操作。但是用户可以通过查询生成新表或者将查询结果导入到文件中。同时，因为Hadoop是面向批处理的系统，而MapReduce任务（job）的启动过程需要消耗较长的时间，所以Hive查询延时比较严重。传统数据库中在秒级别可以完成的查询，在Hive中，即使数据集相对较小，往往也需要执行更长的时间。 Hive架构原理 1）用户接口：Client&emsp; CLI（command-line interface）、JDBC/ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）2）元数据：Metastore&emsp; 元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；&emsp; 默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore3）Hadoop&emsp; 使用HDFS进行存储，使用MapReduce进行计算。4）驱动器：Driver&emsp; （1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。&emsp; （2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。&emsp; （3）优化器（Query Optimizer）：对逻辑执行计划进行优化。&emsp; （4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。 &emsp; Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。 Hive面试题整理（一）Hive表关联查询，如何解决数据倾斜的问题？（☆☆☆☆☆）&emsp; 1）倾斜原因：map输出数据按key Hash的分配到reduce中，由于key分布不均匀、业务数据本身的特、建表时考虑不周、等原因造成的reduce 上的数据量差异过大。&emsp; （1）key分布不均匀;&emsp; （2）业务数据本身的特性;&emsp; （3）建表时考虑不周;&emsp; （4）某些SQL语句本身就有数据倾斜;&emsp; 如何避免：对于key为空产生的数据倾斜，可以对其赋予一个随机值。&emsp; 2）解决方案&emsp; （1）参数调节：&emsp; &emsp; hive.map.aggr = true&emsp; &emsp; hive.groupby.skewindata=true&emsp; 有数据倾斜的时候进行负载均衡，当选项设定位true,生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个Reduce中），最后完成最终的聚合操作。&emsp; （2）SQL 语句调节：&emsp; ① 选用join key分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表做join 的时候，数据量相对变小的效果。&emsp; ② 大小表Join：&emsp; &emsp; 使用map join让小的维度表（1000 条以下的记录条数）先进内存。在map端完成reduce。&emsp; ③ 大表Join大表：&emsp; &emsp; 把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null 值关联不上，处理后并不影响最终结果。&emsp; ④ count distinct大量相同特殊值:&emsp; &emsp; count distinct 时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。 Hive的HSQL转换为MapReduce的过程？（☆☆☆☆☆）&emsp; HiveSQL -&gt;AST(抽象语法树) -&gt; QB(查询块) -&gt;OperatorTree（操作树）-&gt;优化后的操作树-&gt;mapreduce任务树-&gt;优化后的mapreduce任务树 &emsp; 过程描述如下：&emsp; &emsp; SQL Parser：Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree；&emsp; &emsp; Semantic Analyzer：遍历AST Tree，抽象出查询的基本组成单元QueryBlock；&emsp; &emsp; Logical plan：遍历QueryBlock，翻译为执行操作树OperatorTree；&emsp; &emsp; Logical plan optimizer: 逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量；&emsp; &emsp; Physical plan：遍历OperatorTree，翻译为MapReduce任务；&emsp; &emsp; Logical plan optimizer：物理层优化器进行MapReduce任务的变换，生成最终的执行计划。 Hive底层与数据库交互原理？（☆☆☆☆☆）&emsp; 由于Hive的元数据可能要面临不断地更新、修改和读取操作，所以它显然不适合使用Hadoop文件系统进行存储。目前Hive将元数据存储在RDBMS中，比如存储在MySQL、Derby中。元数据信息包括：存在的表、表的列、权限和更多的其他信息。 Hive的两张表关联，使用MapReduce怎么实现？（☆☆☆☆☆）&emsp; 如果其中有一张表为小表，直接使用map端join的方式（map端加载小表）进行聚合。&emsp; 如果两张都是大表，那么采用联合key，联合key的第一个组成部分是join on中的公共字段，第二部分是一个flag，0代表表A，1代表表B，由此让Reduce区分客户信息和订单信息；在Mapper中同时处理两张表的信息，将join on公共字段相同的数据划分到同一个分区中，进而传递到一个Reduce中，然后在Reduce中实现聚合。 请谈一下Hive的特点，Hive和RDBMS有什么异同？&emsp; hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析，但是Hive不支持实时查询。&emsp; Hive与关系型数据库的区别： 请说明hive中 Sort By，Order By，Cluster By，Distrbute By各代表什么意思？&emsp; order by：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。&emsp; sort by：不是全局排序，其在数据进入reducer前完成排序。&emsp; distribute by：按照指定的字段对数据进行划分输出到不同的reduce中。&emsp; cluster by：除了具有 distribute by 的功能外还兼具 sort by 的功能。 写出hive中split、coalesce及collect_list函数的用法（可举例）？&emsp; split将字符串转化为数组，即：split(‘a,b,c,d’ , ‘,’) ==&gt; [“a”,”b”,”c”,”d”]。&emsp; coalesce(T v1, T v2, …) 返回参数中的第一个非空值；如果所有值都为 NULL，那么返回NULL。&emsp; collect_list列出该字段所有的值，不去重 =&gt; select collect_list(id) from table。 Hive有哪些方式保存元数据，各有哪些特点？&emsp; Hive支持三种不同的元存储服务器，分别为：内嵌式元存储服务器、本地元存储服务器、远程元存储服务器，每种存储方式使用不同的配置参数。&emsp; 内嵌式元存储主要用于单元测试，在该模式下每次只有一个进程可以连接到元存储，Derby是内嵌式元存储的默认数据库。&emsp; 在本地模式下，每个Hive客户端都会打开到数据存储的连接并在该连接上请求SQL查询。&emsp; 在远程模式下，所有的Hive客户端都将打开一个到元数据服务器的连接，该服务器依次查询元数据，元数据服务器和客户端之间使用Thrift协议通信。 Hive内部表和外部表的区别？&emsp; 创建表时：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。&emsp; 删除表时：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。 Hive 中的压缩格式TextFile、SequenceFile、RCfile 、ORCfile各有什么区别？&emsp; 1、TextFile&emsp; 默认格式，存储方式为行存储，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，压缩后的文件不支持split，Hive不会对数据进行切分，从而无法对数据进行并行操作。并且在反序列化过程中，必须逐个字符判断是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍。&emsp; 2、SequenceFile&emsp; SequenceFile是Hadoop API提供的一种二进制文件支持，存储方式为行存储，其具有使用方便、可分割、可压缩的特点。&emsp; SequenceFile支持三种压缩选择：NONE，RECORD，BLOCK。Record压缩率低，一般建议使用BLOCK压缩。&emsp; 优势是文件和hadoop api中的MapFile是相互兼容的&emsp; 3、RCFile&emsp; 存储方式：数据按行分块，每块按列存储。结合了行存储和列存储的优点：&emsp; &emsp; 首先，RCFile 保证同一行的数据位于同一节点，因此元组重构的开销很低；&emsp; &emsp; 其次，像列存储一样，RCFile 能够利用列维度的数据压缩，并且能跳过不必要的列读取；&emsp; 4、ORCFile&emsp; 存储方式：数据按行分块 每块按照列存储。&emsp; 压缩快、快速列存取。&emsp; 效率比rcfile高，是rcfile的改良版本。&emsp; 总结：相比TEXTFILE和SEQUENCEFILE，RCFILE由于列式存储方式，数据加载时性能消耗较大，但是具有较好的压缩比和查询响应。&emsp; 数据仓库的特点是一次写入、多次读取，因此，整体来看，RCFILE相比其余两种格式具有较明显的优势。 所有的Hive任务都会有MapReduce的执行吗？&emsp; 不是，从Hive0.10.0版本开始，对于简单的不需要聚合的类似SELECT from LIMIT n语句，不需要起MapReduce job，直接通过Fetch task获取数据。 Hive的函数：UDF、UDAF、UDTF的区别？&emsp; UDF：单行进入，单行输出&emsp; UDAF：多行进入，单行输出&emsp; UDTF：单行输入，多行输出 说说对Hive桶表的理解？&emsp; 桶表是对数据进行哈希取值，然后放到不同文件中存储。&emsp; 数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。物理上，每个桶就是表(或分区）目录里的一个文件，一个作业产生的桶(输出文件)和reduce任务个数相同。&emsp; 桶表专门用于抽样查询，是很专业性的，不是日常用来存储数据的表，需要抽样查询时，才创建和使用桶表。 Hive面试题整理（二）Fetch抓取&emsp; Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。&emsp; 在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。 本地模式&emsp; 大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。&emsp; 用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。 表的优化小表、大表Join&emsp; 将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。&emsp; 实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。 大表Join大表1）空KEY过滤&emsp; 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空。2）空key转换&emsp; 有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。 Group By&emsp; 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。&emsp; 并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。1）开启Map端聚合参数设置&emsp; &emsp; （1）是否在Map端进行聚合，默认为True&emsp; &emsp; &emsp; hive.map.aggr = true&emsp; &emsp; （2）在Map端进行聚合操作的条目数目&emsp; &emsp; &emsp; hive.groupby.mapaggr.checkinterval = 100000&emsp; &emsp; （3）有数据倾斜的时候进行负载均衡（默认是false）&emsp; &emsp; &emsp; hive.groupby.skewindata = true&emsp; 当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。 Count(Distinct) 去重统计&emsp; 数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换 笛卡尔积&emsp; 尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积 行列过滤&emsp; 列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。&emsp; 行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。 数据倾斜Map数1）通常情况下，作业会通过input的目录产生一个或者多个map任务。&emsp; 主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。2）是不是map数越多越好？&emsp; 答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。3）是不是保证每个map处理接近128m的文件块，就高枕无忧了？&emsp; 答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。&emsp; 针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数； 小文件进行合并&emsp; 在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。&emsp; set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 复杂文件增加Map数&emsp; 当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。&emsp; 增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。 Reduce数1）调整reduce个数方法一&emsp; （1）每个Reduce处理的数据量默认是256MB&emsp; &emsp; hive.exec.reducers.bytes.per.reducer=256000000&emsp; （2）每个任务最大的reduce数，默认为1009&emsp; &emsp; hive.exec.reducers.max=1009&emsp; （3）计算reducer数的公式&emsp; &emsp; N=min(参数2，总输入数据量/参数1)2）调整reduce个数方法二&emsp; 在hadoop的mapred-default.xml文件中修改&emsp; 设置每个job的Reduce个数&emsp; set mapreduce.job.reduces = 15;3）reduce个数并不是越多越好&emsp; （1）过多的启动和初始化reduce也会消耗时间和资源；&emsp; （2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；&emsp; 在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适。 ========================== 并行执行&emsp; Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。&emsp; 通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。 Kafka介绍下Kafka，Kafka的作用？Kafka的组件？适用场景？Kafka是一种分布式、高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据，主要应用于大数据实时处理领域。简单地说，Kafka就相比是一个邮箱，生产者是发送邮件的人，消费者是接收邮件的人，Kafka就是用来存东西的，只不过它提供了一些处理邮件的机制。 1作用 1）发布和订阅消息流 2）以容错的方式记录消息流，kafka以文件的方式来存储消息流 3）可以在消息发布的时候进行处理 2、优势 高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒； 可扩展性：kafka集群支持热扩展； 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失； 容错性：允许集群中节点故障（若副本数量为n,则允许n-1个节点故障）； 高并发：支持数千个客户端同时读写。 3、组件 Topic ：可以理解为一个队列，生产者和消费者面向的都是一个 topic； Producer ：消息生产者，就是向 kafka broker 发消息的客户端； Consumer：消息消费者，向 kafka broker 取消息的客户端； Broker ：一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker可以容纳多个 topic。 4、使用场景 1）在系统或应用程序之间构建可靠的用于传输实时数据的管道，消息队列功能 2）构建实时的流数据处理程序来变换或处理数据流，数据处理功能 通俗一点来说 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer； 消息系统：解耦生产者和消费者、缓存消息等； 用户活动跟踪：kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后消费者通过订阅这些topic来做实时的监控分析，亦可保存到数据库； 运营指标：kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告； 流式处理：比如Spark streaming和Flink。 Kafka面试题总结（一）Kafka 都有哪些特点？&emsp; 高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic可以分多个partition, consumer group 对partition进行consume操作。&emsp; 可扩展性：kafka集群支持热扩展&emsp; 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失&emsp; 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）&emsp; 高并发：支持数千个客户端同时读写 请简述下你在哪些场景下会选择 Kafka？&emsp; 日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、HBase、Solr等。&emsp; 消息系统：解耦和生产者和消费者、缓存消息等。&emsp; 用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。&emsp; 运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。&emsp; 流式处理：比如spark streaming和 Flink Kafka 的设计架构？简单架构如下： 详细架构如下： Kafka 架构分为以下几个部分：&emsp; Producer：消息生产者，就是向 kafka broker 发消息的客户端。&emsp; Consumer：消息消费者，向 kafka broker 取消息的客户端。&emsp; Topic：可以理解为一个队列，一个 Topic 又分为一个或多个分区。&emsp; Consumer Group：这是 kafka 用来实现一个 topic 消息的广播（发给所有的 consumer）和单播（发给任意一个 consumer）的手段。一个 topic 可以有多个 Consumer Group。&emsp; Broker：一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker 可以容纳多个 topic。&emsp; Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker上，每个 partition 是一个有序的队列。partition 中的每条消息都会被分配一个有序的id（offset）。将消息发给 consumer，kafka 只保证按一个 partition 中的消息的顺序，不保证一个 topic 的整体（多个 partition 间）的顺序。&emsp; Offset：kafka 的存储文件都是按照 offset.kafka 来命名，用 offset 做名字的好处是方便查找。例如你想找位于 2049 的位置，只要找到 2048.kafka 的文件即可。当然 the first offset 就是 00000000000.kafka。 Kafka 分区的目的？&emsp; 分区对于 Kafka 集群的好处是：实现负载均衡。分区对于消费者来说，可以提高并发度，提高效率。 Kafka 是如何做到消息的有序性？&emsp; kafka 中的每个 partition 中的消息在写入时都是有序的，而且单独一个 partition 只能由一个消费者去消费，可以在里面保证消息的顺序性。但是分区之间的消息是不保证有序的。 Kafka 的高可靠性是怎么实现的？可回答：&emsp; Kafka 在什么情况下会出现消息丢失？1）数据可靠性（可回答 怎么尽可能保证 Kafka 的可靠性？）&emsp; Kafka 作为一个商业级消息中间件，消息可靠性的重要性可想而知。本文从 Producter 往 Broker 发送消息、Topic 分区副本以及 Leader 选举几个角度介绍数据的可靠性。&emsp; Topic分区副本&emsp; 在 Kafka 0.8.0 之前，Kafka 是没有副本的概念的，那时候人们只会用 Kafka 存储一些不重要的数据，因为没有副本，数据很可能会丢失。但是随着业务的发展，支持副本的功能越来越强烈，所以为了保证数据的可靠性，Kafka 从 0.8.0 版本开始引入了分区副本（详情请参见 KAFKA-50）。也就是说每个分区可以人为的配置几个副本（比如创建主题的时候指定 replication-factor，也可以在 Broker 级别进行配置 default.replication.factor），一般会设置为3。&emsp; Kafka 可以保证单个分区里的事件是有序的，分区可以在线（可用），也可以离线（不可用）。在众多的分区副本里面有一个副本是 Leader，其余的副本是 follower，所有的读写操作都是经过 Leader 进行的，同时 follower 会定期地去 leader 上的复制数据。当 Leader 挂了的时候，其中一个 follower 会重新成为新的 Leader。通过分区副本，引入了数据冗余，同时也提供了 Kafka 的数据可靠性。&emsp; Kafka 的分区多副本架构是 Kafka 可靠性保证的核心，把消息写入多个副本可以使 Kafka 在发生崩溃时仍能保证消息的持久性。&emsp; Producer 往 Broker 发送消息&emsp; 如果我们要往 Kafka 对应的主题发送消息，我们需要通过 Producer 完成。前面我们讲过 Kafka 主题对应了多个分区，每个分区下面又对应了多个副本；为了让用户设置数据可靠性， Kafka 在 Producer 里面提供了消息确认机制。也就是说我们可以通过配置来决定消息发送到对应分区的几个副本才算消息发送成功。可以在定义 Producer 时通过 acks 参数指定（在 0.8.2.X 版本之前是通过 request.required.acks 参数设置的）。&emsp; 这个参数支持以下三种值：&emsp; acks = 0：意味着如果生产者能够通过网络把消息发送出去，那么就认为消息已成功写入Kafka。在这种情况下还是有可能发生错误，比如发送的对象无能被序列化或者网卡发生故障，但如果是分区离线或整个集群长时间不可用，那就不会收到任何错误。在 acks=0 模式下的运行速度是非常快的（这就是为什么很多基准测试都是基于这个模式），你可以得到惊人的吞吐量和带宽利用率，不过如果选择了这种模式， 一定会丢失一些消息。&emsp; acks = 1：意味若 Leader 在收到消息并把它写入到分区数据文件（不一定同步到磁盘上）时会返回确认或错误响应。在这个模式下，如果发生正常的 Leader 选举，生产者会在选举时收到一个 LeaderNotAvailableException 异常，如果生产者能恰当地处理这个错误，它会重试发送悄息，最终消息会安全到达新的 Leader 那里。不过在这个模式下仍然有可能丢失数据，比如消息已经成功写入 Leader，但在消息被复制到 follower 副本之前 Leader发生崩溃。&emsp; acks = all（这个和 request.required.acks = -1 含义一样）：意味着 Leader 在返回确认或错误响应之前，会等待所有同步副本都收到悄息。如果和 min.insync.replicas 参数结合起来，就可以决定在返回确认前至少有多少个副本能够收到悄息，生产者会一直重试直到消息被成功提交。不过这也是最慢的做法，因为生产者在继续发送其他消息之前需要等待所有副本都收到当前的消息。&emsp; 根据实际的应用场景，我们设置不同的 acks，以此保证数据的可靠性。&emsp; 另外，Producer 发送消息还可以选择同步（默认，通过 producer.type=sync 配置） 或者异步（producer.type=async）模式。如果设置成异步，虽然会极大的提高消息发送的性能，但是这样会增加丢失数据的风险。如果需要确保消息的可靠性，必须将 producer.type 设置为 sync。&emsp; Leader 选举&emsp; 在介绍 Leader 选举之前，让我们先来了解一下 ISR（in-sync replicas）列表。每个分区的 leader 会维护一个 ISR 列表，ISR 列表里面就是 follower 副本的 Borker 编号，只有跟得上 Leader 的 follower 副本才能加入到 ISR 里面，这个是通过 replica.lag.time.max.ms 参数配置的。只有 ISR 里的成员才有被选为 leader 的可能。2）数据一致性（可回答 Kafka数据一致性原理？）&emsp; 这里介绍的数据一致性主要是说不论是老的 Leader 还是新选举的 Leader，Consumer 都能读到一样的数据。那么 Kafka 是如何实现的呢？ &emsp; 假设分区的副本为3，其中副本0是 Leader，副本1和副本2是 follower，并且在 ISR 列表里面。虽然副本0已经写入了 Message4，但是 Consumer 只能读取到 Message2。因为所有的 ISR 都同步了 Message2，只有 High Water Mark 以上的消息才支持 Consumer 读取，而 High Water Mark 取决于 ISR 列表里面偏移量最小的分区，对应于上图的副本2，这个很类似于木桶原理。&emsp; 这样做的原因是还没有被足够多副本复制的消息被认为是“不安全”的，如果 Leader 发生崩溃，另一个副本成为新 Leader，那么这些消息很可能丢失了。如果我们允许消费者读取这些消息，可能就会破坏一致性。试想，一个消费者从当前 Leader（副本0） 读取并处理了 Message4，这个时候 Leader 挂掉了，选举了副本1为新的 Leader，这时候另一个消费者再去从新的 Leader 读取消息，发现这个消息其实并不存在，这就导致了数据不一致性问题。&emsp; 当然，引入了 High Water Mark 机制，会导致 Broker 间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长（因为我们会先等待消息复制完毕）。延迟时间可以通过参数 replica.lag.time.max.ms 参数配置，它指定了副本在复制消息时可被允许的最大延迟时间。 ISR、OSR、AR 是什么？&emsp; ISR：In-Sync Replicas 副本同步队列&emsp; OSR：Out-of-Sync Replicas&emsp; AR：Assigned Replicas 所有副本&emsp; ISR是由leader维护，follower从leader同步数据有一些延迟（具体可以参见 图文了解 Kafka 的副本复制机制），超过相应的阈值会把 follower 剔除出 ISR, 存入OSR（Out-of-Sync Replicas ）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。 LEO、HW、LSO、LW等分别代表什么？&emsp; LEO：是 LogEndOffset 的简称，代表当前日志文件中下一条&emsp; HW：水位或水印（watermark）一词，也可称为高水位(high watermark)，通常被用在流式处理领域（比如Apache Flink、Apache Spark等），以表征元素或事件在基于时间层面上的进度。在Kafka中，水位的概念反而与时间无关，而是与位置信息相关。严格来说，它表示的就是位置信息，即位移（offset）。取 partition 对应的 ISR中 最小的 LEO 作为 HW，consumer 最多只能消费到 HW 所在的位置上一条信息。&emsp; LSO：是 LastStableOffset 的简称，对未完成的事务而言，LSO 的值等于事务中第一条消息的位置(firstUnstableOffset)，对已完成的事务而言，它的值同 HW 相同&emsp; LW：Low Watermark 低水位, 代表 AR 集合中最小的 logStartOffset 值。 数据传输的事务有几种？&emsp; 数据传输的事务定义通常有以下三种级别：&emsp; 最多一次：消息不会被重复发送，最多被传输一次，但也有可能一次不传输&emsp; 最少一次：消息不会被漏发送，最少被传输一次，但也有可能被重复传输&emsp; 精确的一次（Exactly once）：不会漏传输也不会重复传输，每个消息都传输被接收 Kafka 消费者是否可以消费指定分区消息？&emsp; Kafa consumer消费消息时，向broker发出fetch请求去消费特定分区的消息，consumer指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer拥有了offset的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的。 Kafka消息是采用Pull模式，还是Push模式？&emsp; Kafka最初考虑的问题是，customer应该从brokes拉取消息还是brokers将消息推送到consumer，也就是pull还push。在这方面，Kafka遵循了一种大部分消息系统共同的传统的设计：producer将消息推送到broker，consumer从broker拉取消息。&emsp; 一些消息系统比如Scribe和Apache Flume采用了push模式，将消息推送到下游的consumer。这样做有好处也有坏处：由broker决定消息推送的速率，对于不同消费速率的consumer就不太好处理了。消息系统都致力于让consumer以最大的速率最快速的消费消息，但不幸的是，push模式下，当broker推送的速率远大于consumer消费的速率时，consumer恐怕就要崩溃了。最终Kafka还是选取了传统的pull模式。&emsp; Pull模式的另外一个好处是consumer可以自主决定是否批量的从broker拉取数据。Push模式必须在不知道下游consumer消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免consumer崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。Pull模式下，consumer就可以根据自己的消费能力去决定这些策略。 Pull有个缺点是，如果broker没有可供消费的消息，将导致consumer不断在循环中轮询，直到新消息到t达。为了避免这点，Kafka有个参数可以让consumer阻塞知道新消息到达(当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发送） Kafka 高效文件存储设计特点？&emsp; 1）Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。&emsp; 2）通过索引信息可以快速定位message和确定response的最大大小。&emsp; 3）通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。&emsp; 4）通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 Kafka创建Topic时如何将分区放置到不同的Broker中？&emsp; 1）副本因子不能大于 Broker 的个数；&emsp; 2）第一个分区（编号为0）的第一个副本放置位置是随机从 brokerList 选择的；&emsp; 3）其他分区的第一个副本放置位置相对于第0个分区依次往后移。也就是如果我们有5个 Broker，5个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个 Broker 上，依次类推；&emsp; 4）剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的； Kafka新建的分区会在哪个目录下创建？&emsp; 我们知道，在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘上用于提高读写性能。当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。&emsp; 如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个目录下创建文件夹用于存放数据。&emsp; 但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic名+分区ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。 谈一谈 Kafka 的再均衡&emsp; 在Kafka中，当有新消费者加入或者订阅的topic数发生变化时，会触发Rebalance(再均衡：在同一个消费者组当中，分区的所有权从一个消费者转移到另外一个消费者)机制，Rebalance顾名思义就是重新均衡消费者消费。Rebalance的过程如下：&emsp; 第一步：所有成员都向coordinator发送请求，请求入组。一旦所有成员都发送了请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader。&emsp; 第二步：leader开始分配消费方案，指明具体哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案发给coordinator。coordinator接收到分配方案之后会把方案发给各个consumer，这样组内的所有成员就都知道自己应该消费哪些分区了。&emsp; 所以对于Rebalance来说，Coordinator起着至关重要的作用。 Kafka分区分配策略 &emsp; 在 Kafka 内部存在两种默认的分区分配策略：Range 和 RoundRobin。当以下事件发生时，Kafka 将会进行一次分区分配：&emsp; 1）同一个 Consumer Group 内新增消费者&emsp; 2）消费者离开当前所属的Consumer Group，包括shuts down 或 crashes&emsp; 3）订阅的主题新增分区&emsp; 将分区的所有权从一个消费者移到另一个消费者称为重新平衡（rebalance），如何rebalance就涉及到下面提到的分区分配策略。下面我们将详细介绍 Kafka 内置的两种分区分配策略。本文假设我们有个名为 T1 的主题，其包含了10个分区，然后我们有两个消费者（C1，C2）来消费这10个分区里面的数据，而且 C1 的 num.streams = 1，C2 的 num.streams = 2。&emsp; Range strategy&emsp; Range策略是对每个主题而言的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。在我们的例子里面，排完序的分区将会是0, 1, 2, 3, 4, 5, 6, 7, 8, 9；消费者线程排完序将会是C1-0, C2-0, C2-1。然后将partitions的个数除于消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。&emsp; 在我们的例子里面，我们有10个分区，3个消费者线程，10 / 3 = 3，而且除不尽，那么消费者线程 C1-0 将会多消费一个分区，所以最后分区分配的结果看起来是这样的：&emsp; C1-0 将消费 0, 1, 2, 3 分区&emsp; C2-0 将消费 4, 5, 6 分区&emsp; C2-1 将消费 7, 8, 9 分区&emsp; 假如我们有11个分区，那么最后分区分配的结果看起来是这样的：&emsp; C1-0 将消费 0, 1, 2, 3 分区&emsp; C2-0 将消费 4, 5, 6, 7 分区&emsp; C2-1 将消费 8, 9, 10 分区&emsp; 假如我们有2个主题(T1和T2)，分别有10个分区，那么最后分区分配的结果看起来是这样的：&emsp; C1-0 将消费 T1主题的 0, 1, 2, 3 分区以及 T2主题的 0, 1, 2, 3分区&emsp; C2-0 将消费 T1主题的 4, 5, 6 分区以及 T2主题的 4, 5, 6分区&emsp; C2-1 将消费 T1主题的 7, 8, 9 分区以及 T2主题的 7, 8, 9分区&emsp; 可以看出，C1-0 消费者线程比其他消费者线程多消费了2个分区，这就是Range strategy的一个很明显的弊端。&emsp; RoundRobin strategy&emsp; 使用RoundRobin策略有两个前提条件必须满足：&emsp; 同一个Consumer Group里面的所有消费者的num.streams必须相等；&emsp; 每个消费者订阅的主题必须相同。&emsp; 所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，这里文字可能说不清，看下面的代码应该会明白： 12345678910111213val allTopicPartitions = ctx.partitionsForTopic.flatMap &#123; case(topic, partitions) =&gt; info(&quot;Consumer %s rebalancing the following partitions for topic %s: %s&quot; .format(ctx.consumerId, topic, partitions)) partitions.map(partition =&gt; &#123; TopicAndPartition(topic, partition) &#125;)&#125;.toSeq.sortWith((topicPartition1, topicPartition2) =&gt; &#123; /* * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending * up on one consumer (if it has a high enough stream count). */ topicPartition1.toString.hashCode &lt; topicPartition2.toString.hashCode&#125;) &emsp; 最后按照round-robin风格将分区分别分配给不同的消费者线程。&emsp; 在我们的例子里面，假如按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9，我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1，最后分区分配的结果为：&emsp; C1-0 将消费 T1-5, T1-2, T1-6 分区；&emsp; C1-1 将消费 T1-3, T1-1, T1-9 分区；&emsp; C2-0 将消费 T1-0, T1-4 分区；&emsp; C2-1 将消费 T1-8, T1-7 分区。&emsp; 多个主题的分区分配和单个主题类似。 Kafka 是如何实现高吞吐率的？&emsp; Kafka是分布式消息系统，需要处理海量的消息，Kafka的设计是把所有的消息都写入速度低容量大的硬盘，以此来换取更强的存储能力，但实际上，使用硬盘并没有带来过多的性能损失。kafka主要使用了以下几个方式实现了超高的吞吐率：&emsp; 1）顺序读写&emsp; 2）零拷贝&emsp; 3）文件分段&emsp; 4）批量发送&emsp; 5）数据压缩 Kafka 缺点？&emsp; 1）由于是批量发送，数据并非真正的实时；&emsp; 2）对于mqtt协议不支持；&emsp; 3）不支持物联网传感数据直接接入；&emsp; 4）仅支持统一分区内消息有序，无法实现全局消息有序；&emsp; 5）监控不完善，需要安装插件；&emsp; 6）依赖zookeeper进行元数据管理。 Kafka 新旧消费者的区别？&emsp; 旧的 Kafka 消费者 API 主要包括：SimpleConsumer（简单消费者） 和 ZookeeperConsumerConnectir（高级消费者）。SimpleConsumer 名字看起来是简单消费者，但是其实用起来很不简单，可以使用它从特定的分区和偏移量开始读取消息。高级消费者和现在新的消费者有点像，有消费者群组，有分区再均衡，不过它使用 ZK 来管理消费者群组，并不具备偏移量和再均衡的可操控性。&emsp; 现在的消费者同时支持以上两种行为，所以为啥还用旧消费者 API 呢？ Kafka 分区数可以增加或减少吗？为什么？&emsp; 我们可以使用 bin/kafka-topics.sh 命令对 Kafka 增加 Kafka 的分区数据，但是 Kafka 不支持减少分区数。 Kafka 分区数据不支持减少是由很多原因的，比如减少的分区其数据放到哪里去？是删除，还是保留？删除的话，那么这些没消费的消息不就丢了。如果保留这些消息如何放到其他分区里面？追加到其他分区后面的话那么就破坏了 Kafka 单个分区的有序性。如果要保证删除分区数据插入到其他分区保证有序性，那么实现起来逻辑就会非常复杂。 Kafka面试题整理（二）请说明什么是Apache Kafka？&emsp; Apache Kafka是由Apache开发的一种发布订阅消息系统，它是一个分布式的、分区的和重复的日志服务。 请说明什么是传统的消息传递方法？&emsp; 传统的消息传递方法包括两种：&emsp; &emsp; 队列：在队列中，一组用户可以从服务器中读取消息，每条消息都发送给其中一个人。&emsp; &emsp; 发布-订阅：在这个模型中，消息被广播给所有的用户。 请说明Kafka相对于传统的消息传递方法有什么优势？&emsp; 高性能：单一的Kafka代理可以处理成千上万的客户端，每秒处理数兆字节的读写操作，Kafka性能远超过传统的ActiveMQ、RabbitMQ等，而且Kafka支持Batch操作；&emsp; 可扩展：Kafka集群可以透明的扩展，增加新的服务器进集群；&emsp; 容错性： Kafka每个Partition数据会复制到几台服务器，当某个Broker失效时，Zookeeper将通知生产者和消费者从而使用其他的Broker。 在Kafka中broker的意义是什么？&emsp; 在Kafka集群中，broker指Kafka服务器。&emsp; 术语解析： Kafka服务器能接收到的最大信息是多少？&emsp; Kafka服务器可以接收到的消息的最大大小是1000000字节。 Kafka中的ZooKeeper是什么？Kafka是否可以脱离ZooKeeper独立运行？&emsp; Zookeeper是一个开放源码的、高性能的协调服务，它用于Kafka的分布式应用。&emsp; 不可以，不可能越过Zookeeper直接联系Kafka broker，一旦Zookeeper停止工作，它就不能服务客户端请求。&emsp; Zookeeper主要用于在集群中不同节点之间进行通信，在Kafka中，它被用于提交偏移量，因此如果节点在任何情况下都失败了，它都可以从之前提交的偏移量中获取，除此之外，它还执行其他活动，如: leader检测、分布式同步、配置管理、识别新节点何时离开或连接、集群、节点实时状态等等。 解释Kafka的用户如何消费信息？&emsp; 在Kafka中传递消息是通过使用sendfile API完成的。它支持将字节Socket转移到磁盘，通过内核空间保存副本，并在内核用户之间调用内核。 解释如何提高远程用户的吞吐量？&emsp; 如果用户位于与broker不同的数据中心，则可能需要调优Socket缓冲区大小，以对长网络延迟进行摊销。 解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息？&emsp; 在数据中，为了精确地获得Kafka的消息，你必须遵循两件事：在数据消耗期间避免重复，在数据生产过程中避免重复。&emsp; 这里有两种方法，可以在数据生成时准确地获得一个语义:&emsp; 每个分区使用一个单独的写入器，每当你发现一个网络错误，检查该分区中的最后一条消息，以查看您的最后一次写入是否成功。&emsp; 在消息中包含一个主键(UUID或其他)，并在用户中进行反复制。 解释如何减少ISR中的扰动？broker什么时候离开ISR？（☆☆☆☆☆）&emsp; ISR是一组与leaders完全同步的消息副本，也就是说ISR中包含了所有提交的消息。ISR应该总是包含所有的副本，直到出现真正的故障。如果一个副本从leader中脱离出来，将会从ISR中删除。 Kafka为什么需要复制？&emsp; Kafka的信息复制确保了任何已发布的消息不会丢失，并且可以在机器错误、程序错误或更常见些的软件升级中使用。 如果副本在ISR中停留了很长时间表明什么？&emsp; 如果一个副本在ISR中保留了很长一段时间，那么它就表明，跟踪器无法像在leader收集数据那样快速地获取数据。 请说明如果首选的副本不在ISR中会发生什么？&emsp; 如果首选的副本不在ISR中，控制器将无法将leadership转移到首选的副本。 Kafka有可能在生产后发生消息偏移吗？&emsp; 在大多数队列系统中，作为生产者的类无法做到这一点，它的作用是触发并忘记消息。broker将完成剩下的工作，比如使用id进行适当的元数据处理、偏移量等。&emsp; 作为消息的用户，你可以从Kafka broker中获得补偿。如果你注视SimpleConsumer类，你会注意到它会获取包括偏移量作为列表的MultiFetchResponse对象。此外，当你对Kafka消息进行迭代时，你会拥有包括偏移量和消息发送的MessageAndOffset对象。 请说明Kafka 的消息投递保证（delivery guarantee）机制以及如何实现？（☆☆☆☆☆）&emsp; Kafka支持三种消息投递语义：&emsp; ① At most once 消息可能会丢，但绝不会重复传递&emsp; ② At least one 消息绝不会丢，但可能会重复传递&emsp; ③ Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户想要的&emsp; consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中存下该consumer在该partition下读取的消息的offset，该consumer下一次再读该partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。&emsp; 可以将consumer设置为autocommit，即consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际上实际使用中consumer并非读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。&emsp; 读完消息先commit再处理消息。这种模式下，如果consumer在commit后还没来得及处理消息就crash了，下次重新开始工作后就无法读到刚刚已提交而未处理的消息，这就对应于At most once。&emsp; 读完消息先处理再commit消费状态(保存offset)。这种模式下，如果在处理完消息之后commit之前Consumer crash了，下次重新开始工作时还会处理刚刚未commit的消息，实际上该消息已经被处理过了，这就对应于At least once。&emsp; 如果一定要做到Exactly once，就需要协调offset和实际操作的输出。经典的做法是引入两阶段提交，但由于许多输出系统不支持两阶段提交，更为通用的方式是将offset和操作输入存在同一个地方。比如，consumer拿到数据后可能把数据放到HDFS，如果把最新的offset和数据本身一起写到HDFS，那就可以保证数据的输出和offset的更新要么都完成，要么都不完成，间接实现Exactly once。（目前就high level API而言，offset是存于Zookeeper中的，无法存于HDFS，而low level API的offset是由自己去维护的，可以将之存于HDFS中）。&emsp; 总之，Kafka默认保证At least once，并且允许通过设置producer异步提交来实现At most once，而Exactly once要求与目标存储系统协作，Kafka提供的offset可以较为容易地实现这种方式。 如何保证Kafka的消息有序（☆☆☆☆☆）&emsp; Kafka对于消息的重复、丢失、错误以及顺序没有严格的要求。&emsp; Kafka只能保证一个partition中的消息被某个consumer消费时是顺序的，事实上，从Topic角度来说，当有多个partition时，消息仍然不是全局有序的。 kafka数据丢失问题,及如何保证？1）数据丢失：&emsp; acks=1的时候(只保证写入leader成功)，如果刚好leader挂了。数据会丢失。&emsp; acks=0的时候，使用异步模式的时候，该模式下kafka无法保证消息，有可能会丢。2）brocker如何保证不丢失：&emsp; acks=all : 所有副本都写入成功并确认。&emsp; retries = 一个合理值。&emsp; min.insync.replicas=2 消息至少要被写入到这么多副本才算成功。&emsp; unclean.leader.election.enable=false 关闭unclean leader选举，即不允许非ISR中的副本被选举为leader，以避免数据丢失。3）Consumer如何保证不丢失&emsp; 如果在消息处理完成前就提交了offset，那么就有可能造成数据的丢失。&emsp; enabel.auto.commit=false关闭自动提交offset&emsp; 处理完数据之后手动提交。 kafka的balance是怎么做的？&emsp; 生产者将数据发布到他们选择的主题。生产者可以选择在主题中分配哪个分区的消息。这可以通过循环的方式来完成，只是为了平衡负载，或者可以根据一些语义分区功能（比如消息中的一些键）来完成。更多关于分区在一秒钟内的使用。 kafka的消费者方式？&emsp; consumer采用pull（拉）模式从broker中读取数据。&emsp; push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。&emsp; 对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。&emsp; pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞。 HBaseHBase架构 从Hbase的架构图上可以看出，Hbase中的存储包括HMaster、HRegionSever、HRegion、HLog、Store、MemStore、StoreFile、HFile等。 Hbase中的每张表都通过键按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过256M就要被分割成两个，这个过程由HRegionServer管理,而HRegion的分配由HMaster管理。 1）HMaster的作用： 为HRegionServer分配HRegion 负责HRegionServer的负载均衡 发现失效的HRegionServer并重新分配 HDFS上的垃圾文件回收 处理Schema更新请求 2）HRegionServer的作用： 维护HMaster分配给它的HRegion，处理对这些HRegion的IO请求 负责切分正在运行过程中变得过大的HRegion 通过架构可以得知，Client访问Hbase上的数据并不需要HMaster参与，寻址访问ZooKeeper和HRegionServer，数据读写访问HRegionServer，HMaster仅仅维护Table和Region的元数据信息，Table的元数据信息保存在ZooKeeper上，负载很低。HRegionServer存取一个子表时，会创建一个HRegion对象，然后对表的每个列簇创建一个Store对象，每个Store都会有一个MemStore和0或多个StoreFile与之对应，每个StoreFile都会对应一个HFile，HFile就是实际的存储文件。因此，一个HRegion有多少列簇就有多少个Store。 一个HRegionServer会有多个HRegion和一个HLog。 3）HRegion Table在行的方向上分割为多个HRegion，HRegion是Hbase中分布式存储和负载均衡的最小单元，即不同的HRegion可以分别在不同的HRegionServer上，但同一个HRegion是不会拆分到多个HRegionServer上的。HRegion按大小分割，每个表一般只有一个HRegion，随着数据不断插入表，HRegion不断增大，当HRegion的某个列簇达到一个阀值（默认256M）时就会分成两个新的HRegion。 &lt;表名，StartRowKey, 创建时间&gt; 由目录表(-ROOT-和.META.)记录该Region的EndRowKey HRegion被分配给哪个HRegionServer是完全动态的，所以需要机制来定位HRegion具体在哪个HRegionServer，Hbase使用三层结构来定位HRegion： 通过zk里的文件/Hbase/rs得到-ROOT-表的位置。-ROOT-表只有一个region。 通过-ROOT-表查找.META.表的第一个表中相应的HRegion位置。其实-ROOT-表是.META.表的第一个region；.META.表中的每一个Region在-ROOT-表中都是一行记录。 通过.META.表找到所要的用户表HRegion的位置。用户表的每个HRegion在.META.表中都是一行记录。-ROOT-表永远不会被分隔为多个HRegion，保证了最多需要三次跳转，就能定位到任意的region。Client会将查询的位置信息保存缓存起来，缓存不会主动失效，因此如果Client上的缓存全部失效，则需要进行6次网络来回，才能定位到正确的HRegion，其中三次用来发现缓存失效，另外三次用来获取位置信息。 4）Store 每一个HRegion由一个或多个Store组成，至少是一个Store，Hbase会把一起访问的数据放在一个Store里面，即为每个ColumnFamily建一个Store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个MemStore和0或者多个StoreFile组成。 Hbase以Store的大小来判断是否需要切分HRegion。 5）MemStore MemStore 是放在内存里的，保存修改的数据即keyValues。当MemStore的大小达到一个阀值（默认64MB）时，MemStore会被Flush到文件，即生成一个快照。目前Hbase会有一个线程来负责MemStore的Flush操作。 6）StoreFile MemStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。 7）HFile Hbase中KeyValue数据的存储格式，是Hadoop的二进制格式文件。 首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。 Trailer中有指针指向其他数据块的起始点，FileInfo记录了文件的一些meta信息。Data Block是Hbase IO的基本单元，为了提高效率，HRegionServer中有基于LRU的Block Cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定（默认块大小64KB），大号的Block有利于顺序Scan，小号的Block利于随机查询。每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成，Magic内容就是一些随机数字，目的是防止数据损坏，结构如下。 HFile结构图如下： Data Block段用来保存表中的数据，这部分可以被压缩。 Meta Block段（可选的）用来保存用户自定义的kv段，可以被压缩。 FileInfo段用来保存HFile的元信息，不能被压缩，用户也可以在这一部分添加自己的元信息。 Data Block Index段（可选的）用来保存Meta Blcok的索引。 Trailer这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，然后，DataBlock Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key。DataBlock Index采用LRU机制淘汰。 HFile的Data Block，Meta Block通常采用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是需要花费cpu进行压缩和解压缩。（备注：DataBlock Index的缺陷：a) 占用过多内存 b) 启动加载时间缓慢） 8）HLog HLog(WAL log)：WAL意为write ahead log，用来做灾难恢复使用，HLog记录数据的所有变更，一旦region server 宕机，就可以从log中进行恢复。 面试题整理HBase的特点是什么？&emsp; 1）大：一个表可以有数十亿行，上百万列；&emsp; 2）无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列；&emsp; 3）面向列：面向列（族）的存储和权限控制，列（族）独立检索；&emsp; 4）稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；&emsp; 5）数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；&emsp; 6）数据类型单一：Hbase中的数据都是字符串，没有类型。 HBase和Hive的区别？ &emsp; Hive和Hbase是两种基于Hadoop的不同技术—Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。 当然，这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询， 数据也可以从Hive写到Hbase，设置再从Hbase写回Hive。 HBase适用于怎样的情景？&emsp; ① 半结构化或非结构化数据&emsp; 对于数据结构字段不够确定或杂乱无章很难按一个概念去进行抽取的数据适合用HBase。以上面的例子为例，当业务发展需要存储author的email，phone，address信息时RDBMS需要停机维护，而HBase支持动态增加。&emsp; ② 记录非常稀疏&emsp; RDBMS的行有多少列是固定的，为null的列浪费了存储空间。而如上文提到的，HBase为null的Column不会被存储，这样既节省了空间又提高了读性能。&emsp; ③ 多版本数据&emsp; 如上文提到的根据Row key和Column key定位到的Value可以有任意数量的版本值，因此对于需要存储变动历史记录的数据，用HBase就非常方便了。比如上例中的author的Address是会变动的，业务上一般只需要最新的值，但有时可能需要查询到历史值。&emsp; ④ 超大数据量&emsp; 当数据量越来越大，RDBMS数据库撑不住了，就出现了读写分离策略，通过一个Master专门负责写操作，多个Slave负责读操作，服务器成本倍增。随着压力增加，Master撑不住了，这时就要分库了，把关联不大的数据分开部署，一些join查询不能用了，需要借助中间层。随着数据量的进一步增加，一个表的记录越来越大，查询就变得很慢，于是又得搞分表，比如按ID取模分成多个表以减少单个表的记录数。经历过这些事的人都知道过程是多么的折腾。采用HBase就简单了，只需要加机器即可，HBase会自动水平切分扩展，跟Hadoop的无缝集成保障了其数据可靠性（HDFS）和海量数据分析的高性能（MapReduce）。 描述HBase的rowKey的设计原则？（☆☆☆☆☆）（1）Rowkey长度原则&emsp; Rowkey 是一个二进制码流，Rowkey 的长度被很多开发者建议说设计在10~100 个字节，不过建议是越短越好，不要超过16 个字节。&emsp; 原因如下：&emsp; ① 数据的持久化文件HFile 中是按照KeyValue 存储的，如果Rowkey 过长比如100 个字节，1000 万列数据光Rowkey 就要占用100*1000 万=10 亿个字节，将近1G 数据，这会极大影响HFile 的存储效率；&emsp; ② MemStore 将缓存部分数据到内存，如果Rowkey 字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此Rowkey 的字节长度越短越好。&emsp; ③ 目前操作系统是都是64 位系统，内存8 字节对齐。控制在16 个字节，8 字节的整数倍利用操作系统的最佳特性。（2）Rowkey散列原则&emsp; 如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver 实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个 RegionServer 上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。（3）Rowkey唯一原则&emsp; 必须在设计上保证其唯一性。 描述HBase中scan和get的功能以及实现的异同？（☆☆☆☆☆）HBase的查询实现只提供两种方式：&emsp; 1）按指定RowKey 获取唯一一条记录，get方法（org.apache.hadoop.hbase.client.Get） Get 的方法处理分两种 : 设置了ClosestRowBefore 和没有设置ClosestRowBefore的rowlock。主要是用来保证行的事务性，即每个get 是以一个row 来标记的。一个row中可以有很多family 和column。&emsp; 2）按指定的条件获取一批记录，scan方法(org.apache.Hadoop.hbase.client.Scan）实现条件查询功能使用的就是scan 方式。&emsp; &emsp; （1）scan 可以通过setCaching 与setBatch 方法提高速度(以空间换时间)；&emsp; &emsp; （2）scan 可以通过setStartRow 与setEndRow 来限定范围([start，end)start 是闭区间，end 是开区间)。范围越小，性能越高。&emsp; &emsp; （3）scan 可以通过setFilter 方法添加过滤器，这也是分页、多条件查询的基础。 请描述HBase中scan对象的setCache和setBatch方法的使用？（☆☆☆☆☆）&emsp; setCache用于设置缓存，即设置一次RPC请求可以获取多行数据。对于缓存操作，如果行的数据量非常大，多行数据有可能超过客户端进程的内存容量，由此引入批量处理这一解决方案。&emsp; setBatch 用于设置批量处理，批量可以让用户选择每一次ResultScanner实例的next操作要取回多少列，例如，在扫描中设置setBatch(5)，则一次next()返回的Result实例会包括5列。如果一行包括的列数超过了批量中设置的值，则可以将这一行分片，每次next操作返回一片，当一行的列数不能被批量中设置的值整除时，最后一次返回的Result实例会包含比较少的列，如，一行17列，batch设置为5，则一共返回4个Result实例，这4个实例中包括的列数分别为5、5、5、2。&emsp; 组合使用扫描器缓存和批量大小，可以让用户方便地控制扫描一个范围内的行键所需要的RPC调用次数。Cache设置了服务器一次返回的行数，而Batch设置了服务器一次返回的列数。&emsp; 假如我们建立了一张有两个列族的表，添加了10行数据，每个行的每个列族下有10列，这意味着整个表一共有200列（或单元格，因为每个列只有一个版本），其中每行有20列。 &emsp; ① Batch参数决定了一行数据分为几个Result，它只针对一行数据，Batch再大，也只能将一行的数据放入一个Result中。所以当一行数据有10列，而Batch为100时，也只能将一行的所有列都放入一个Result，不会混合其他行；&emsp; ② 缓存值决定一次RPC返回几个Result，根据Batch划分的Result个数除以缓存个数可以得到RPC消息个数（之前定义缓存值决定一次返回的行数，这是不准确的，准确来说是决定一次RPC返回的Result个数，由于在引入Batch之前，一行封装为一个Result，因此定义缓存值决定一次返回的行数，但引入Batch后，更准确的说法是缓存值决定了一次RPC返回的Result个数）；&emsp; &emsp; RPC请求次数 = （行数 * 每行列数） / Min（每行的列数，批量大小） / 扫描器缓存&emsp; 下图展示了缓存和批量两个参数如何联动，下图中有一个包含9行数据的表，每行都包含一些列。使用了一个缓存为6、批量大小为3的扫描器，需要三次RPC请求来传送数据： 请详细描述HBase中一个cell的结构？&emsp; HBase中通过row和columns确定的为一个存贮单元称为cell。&emsp; Cell：由{row key, column(= + ), version}唯一确定的单元。cell 中的数据是没有类型的，全部是字节码形式存贮。 简述HBase中compact用途是什么，什么时候触发，分为哪两种，有什么区别，有哪些相关配置参数？（☆☆☆☆☆）&emsp; 在hbase中每当有memstore数据flush到磁盘之后，就形成一个storefile，当storeFile的数量达到一定程度后，就需要将 storefile 文件来进行 compaction 操作。&emsp; Compact 的作用：&emsp; ① 合并文件&emsp; ② 清除过期，多余版本的数据&emsp; ③ 提高读写数据的效率&emsp; HBase 中实现了两种 compaction 的方式：minor and major. 这两种 compaction 方式的区别是：&emsp; 1）Minor 操作只用来做部分文件的合并操作以及包括 minVersion=0 并且设置 ttl 的过期版本清理，不做任何删除数据、多版本数据的清理工作。&emsp; 2）Major 操作是对 Region 下的HStore下的所有StoreFile执行合并操作，最终的结果是整理合并出一个文件。 每天百亿数据存入HBase，如何保证数据的存储正确和在规定的时间里全部录入完毕，不残留数据？（☆☆☆☆☆）需求分析：&emsp; 1）百亿数据：证明数据量非常大；&emsp; 2）存入HBase：证明是跟HBase的写入数据有关；&emsp; 3）保证数据的正确：要设计正确的数据结构保证正确性；&emsp; 4）在规定时间内完成：对存入速度是有要求的。解决思路：&emsp; 1）数据量百亿条，什么概念呢？假设一整天60x60x24 = 86400秒都在写入数据，那么每秒的写入条数高达100万条，HBase当然是支持不了每秒百万条数据的，所以这百亿条数据可能不是通过实时地写入，而是批量地导入。批量导入推荐使用BulkLoad方式（推荐阅读：Spark之读写HBase），性能是普通写入方式几倍以上；&emsp; 2）存入HBase：普通写入是用JavaAPI put来实现，批量导入推荐使用BulkLoad；&emsp; 3）保证数据的正确：这里需要考虑RowKey的设计、预建分区和列族设计等问题；&emsp; 4）在规定时间内完成也就是存入速度不能过慢，并且当然是越快越好，使用BulkLoad。 请列举几个HBase优化方法？（☆☆☆☆☆）1）减少调整&emsp; 减少调整这个如何理解呢？HBase中有几个内容会动态调整，如region（分区）、HFile，所以通过一些方法来减少这些会带来I/O开销的调整。&emsp; ① Region&emsp; &emsp; 如果没有预建分区的话，那么随着region中条数的增加，region会进行分裂，这将增加I/O开销，所以解决方法就是根据你的RowKey设计来进行预建分区，减少region的动态分裂。&emsp; ② HFile&emsp; &emsp; HFile是数据底层存储文件，在每个memstore进行刷新时会生成一个HFile，当HFile增加到一定程度时，会将属于一个region的HFile进行合并，这个步骤会带来开销但不可避免，但是合并后HFile大小如果大于设定的值，那么HFile会重新分裂。为了减少这样的无谓的I/O开销，建议估计项目数据量大小，给HFile设定一个合适的值。2）减少启停&emsp; 数据库事务机制就是为了更好地实现批量写入，较少数据库的开启关闭带来的开销，那么HBase中也存在频繁开启关闭带来的问题。&emsp; ① 关闭Compaction，在闲时进行手动Compaction。&emsp; &emsp; 因为HBase中存在Minor Compaction和Major Compaction，也就是对HFile进行合并，所谓合并就是I/O读写，大量的HFile进行肯定会带来I/O开销，甚至是I/O风暴，所以为了避免这种不受控制的意外发生，建议关闭自动Compaction，在闲时进行compaction。&emsp; ② 批量数据写入时采用BulkLoad。&emsp; 如果通过HBase-Shell或者JavaAPI的put来实现大量数据的写入，那么性能差是肯定并且还可能带来一些意想不到的问题，所以当需要写入大量离线数据时建议使用BulkLoad。3）减少数据量&emsp; 虽然我们是在进行大数据开发，但是如果可以通过某些方式在保证数据准确性同时减少数据量，何乐而不为呢？&emsp; ① 开启过滤，提高查询速度&emsp; &emsp; 开启BloomFilter，BloomFilter是列族级别的过滤，在生成一个StoreFile同时会生成一个MetaBlock，用于查询时过滤数据&emsp; ② 使用压缩&emsp; &emsp; 一般推荐使用Snappy和LZO压缩4）合理设计&emsp; 在一张HBase表格中RowKey和ColumnFamily的设计是非常重要，好的设计能够提高性能和保证数据的准确性&emsp; ① RowKey设计：应该具备以下几个属性&emsp; &emsp; 散列性：散列性能够保证相同相似的rowkey聚合，相异的rowkey分散，有利于查询。&emsp; &emsp; 简短性：rowkey作为key的一部分存储在HFile中，如果为了可读性将rowKey设计得过长，那么将会增加存储压力。&emsp; &emsp; 唯一性：rowKey必须具备明显的区别性。&emsp; &emsp; 业务性：举例来说：&emsp; &emsp; 假如我的查询条件比较多，而且不是针对列的条件，那么rowKey的设计就应该支持多条件查询。&emsp; &emsp; 如果我的查询要求是最近插入的数据优先，那么rowKey则可以采用叫上Long.Max-时间戳的方式，这样rowKey就是递减排列。&emsp; ② 列族的设计：列族的设计需要看应用场景&emsp; &emsp; 优势：HBase中数据时按列进行存储的，那么查询某一列族的某一列时就不需要全盘扫描，只需要扫描某一列族，减少了读I/O；其实多列族设计对减少的作用不是很明显，适用于读多写少的场景&emsp; &emsp; 劣势：降低了写的I/O性能。原因如下：数据写到store以后是先缓存在memstore中，同一个region中存在多个列族则存在多个store，每个store都一个memstore，当其实memstore进行flush时，属于同一个region的store中的memstore都会进行flush，增加I/O开销。 Region如何预建分区？&emsp; 预分区的目的主要是在创建表的时候指定分区数，提前规划表有多个分区，以及每个分区的区间范围，这样在存储的时候rowkey按照分区的区间存储，可以避免region热点问题。&emsp; 通常有两种方案：&emsp; 方案1：shell 方法&emsp; &emsp; create ‘tb_splits’, {NAME =&gt; ‘cf’,VERSIONS=&gt; 3},{SPLITS =&gt; [‘10’,’20’,’30’]}&emsp; 方案2：JAVA程序控制&emsp; &emsp; ① 取样，先随机生成一定数量的rowkey,将取样数据按升序排序放到一个集合里；&emsp; &emsp; ② 根据预分区的region个数，对整个集合平均分割，即是相关的splitKeys；&emsp; &emsp; ③ HBaseAdmin.createTable(HTableDescriptor tableDescriptor,byte[][]splitkeys)可以指定预分区的splitKey，即是指定region间的rowkey临界值。 HRegionServer宕机如何处理？（☆☆☆☆☆）1）ZooKeeper会监控HRegionServer的上下线情况，当ZK发现某个HRegionServer宕机之后会通知HMaster进行失效备援；2）该HRegionServer会停止对外提供服务，就是它所负责的region暂时停止对外提供服务；3）HMaster会将该HRegionServer所负责的region转移到其他HRegionServer上，并且会对HRegionServer上存在memstore中还未持久化到磁盘中的数据进行恢复；4）这个恢复的工作是由WAL重播来完成，这个过程如下：&emsp; ① wal实际上就是一个文件，存在/hbase/WAL/对应RegionServer路径下。&emsp; ② 宕机发生时，读取该RegionServer所对应的路径下的wal文件，然后根据不同的region切分成不同的临时文件recover.edits。&emsp; ③ 当region被分配到新的RegionServer中，RegionServer读取region时会进行是否存在recover.edits，如果有则进行恢复。 HBase读写流程？（☆☆☆☆☆）&emsp; 读：&emsp; ① HRegionServer保存着meta表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取meta表所在的位置信息，即找到这个meta表在哪个HRegionServer上保存着。&emsp; ② 接着Client通过刚才获取到的HRegionServer的IP来访问Meta表所在的HRegionServer，从而读取到Meta，进而获取到Meta表中存放的元数据。&emsp; ③ Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据。&emsp; ④ 最后HRegionServer把查询到的数据响应给Client。&emsp; 写：&emsp; ① Client先访问zookeeper，找到Meta表，并获取Meta表元数据。&emsp; ② 确定当前将要写入的数据所对应的HRegion和HRegionServer服务器。&emsp; ③ Client向该HRegionServer服务器发起写入数据请求，然后HRegionServer收到请求并响应。&emsp; ④ Client先把数据写入到HLog，以防止数据丢失。&emsp; ⑤ 然后将数据写入到Memstore。&emsp; ⑥ 如果HLog和Memstore均写入成功，则这条数据写入成功。&emsp; ⑦ 如果Memstore达到阈值，会把Memstore中的数据flush到Storefile中。&emsp; ⑧ 当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的Storefile。&emsp; ⑨ 当Storefile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。 HBase内部机制是什么？&emsp; Hbase是一个能适应联机业务的数据库系统&emsp; 物理存储：hbase的持久化数据是将数据存储在HDFS上。&emsp; 存储管理：一个表是划分为很多region的，这些region分布式地存放在很多regionserver上Region内部还可以划分为store，store内部有memstore和storefile。&emsp; 版本管理：hbase中的数据更新本质上是不断追加新的版本，通过compact操作来做版本间的文件合并Region的split。&emsp; 集群管理：ZooKeeper + HMaster + HRegionServer。 Hbase中的memstore是用来做什么的？&emsp; hbase为了保证随机读取的性能，所以hfile里面的rowkey是有序的。当客户端的请求在到达regionserver之后，为了保证写入rowkey的有序性，所以不能将数据立刻写入到hfile中，而是将每个变更操作保存在内存中，也就是memstore中。memstore能够很方便的支持操作的随机插入，并保证所有的操作在内存中是有序的。当memstore达到一定的量之后，会将memstore里面的数据flush到hfile中，这样能充分利用hadoop写入大文件的性能优势，提高写入性能。&emsp; 由于memstore是存放在内存中，如果regionserver因为某种原因死了，会导致内存中数据丢失。所有为了保证数据不丢失，hbase将更新操作在写入memstore之前会写入到一个write ahead log(WAL)中。WAL文件是追加、顺序写入的，WAL每个regionserver只有一个，同一个regionserver上所有region写入同一个的WAL文件。这样当某个regionserver失败时，可以通过WAL文件，将所有的操作顺序重新加载到memstore中。 HBase在进行模型设计时重点在什么地方？一张表中定义多少个Column Family最合适？为什么？（☆☆☆☆☆）&emsp; Column Family的个数具体看表的数据，一般来说划分标准是根据数据访问频度，如一张表里有些列访问相对频繁，而另一些列访问很少，这时可以把这张表划分成两个列族，分开存储，提高访问效率。 如何提高HBase客户端的读写性能？请举例说明（☆☆☆☆☆）&emsp; ① 开启bloomfilter过滤器，开启bloomfilter比没开启要快3、4倍&emsp; ② Hbase对于内存有特别的需求，在硬件允许的情况下配足够多的内存给它&emsp; ③ 通过修改hbase-env.sh中的 export HBASE_HEAPSIZE=3000 #这里默认为1000m&emsp; ④ 增大RPC数量&emsp; &emsp; 通过修改hbase-site.xml中的hbase.regionserver.handler.count属性，可以适当的放大RPC数量，默认值为10有点小。 HBase集群安装注意事项？&emsp; ① HBase需要HDFS的支持，因此安装HBase前确保Hadoop集群安装完成；&emsp; ② HBase需要ZooKeeper集群的支持，因此安装HBase前确保ZooKeeper集群安装完成；&emsp; ③ 注意HBase与Hadoop的版本兼容性；&emsp; ④ 注意hbase-env.sh配置文件和hbase-site.xml配置文件的正确配置；&emsp; ⑤ 注意regionservers配置文件的修改；&emsp; ⑥ 注意集群中的各个节点的时间必须同步，否则启动HBase集群将会报错。 直接将时间戳作为行健，在写入单个region 时候会发生热点问题，为什么呢？（☆☆☆☆☆）&emsp; region中的rowkey是有序存储，若时间比较集中。就会存储到一个region中，这样一个region的数据变多，其它的region数据很少，加载数据就会很慢，直到region分裂，此问题才会得到缓解。 请描述如何解决HBase中region太小和region太大带来的冲突？（☆☆☆☆☆）&emsp; Region过大会发生多次compaction，将数据读一遍并重写一遍到hdfs 上，占用io，region过小会造成多次split，region 会下线，影响访问服务，最佳的解决方法是调整hbase.hregion. max.filesize 为256m。","categories":[],"tags":[{"name":"bigdata","slug":"bigdata","permalink":"http://example.com/tags/bigdata/"},{"name":"面试","slug":"面试","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://example.com/tags/Hadoop/"},{"name":"Hive","slug":"Hive","permalink":"http://example.com/tags/Hive/"},{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"HBase","slug":"HBase","permalink":"http://example.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://example.com/tags/Spark/"}]},{"title":"记录各种踩坑","slug":"记录各种踩坑","date":"2022-01-26T09:53:44.000Z","updated":"2022-01-26T10:04:25.679Z","comments":true,"path":"2022/01/26/记录各种踩坑/","link":"","permalink":"http://example.com/2022/01/26/%E8%AE%B0%E5%BD%95%E5%90%84%E7%A7%8D%E8%B8%A9%E5%9D%91/","excerpt":"","text":"从pdf复制配置pom，maven报红 2022年1月26日 17:59 问题在于从pdf文件复制的横杠（减号）变成了‐，而pom文件中应该是- 所以千万不要再从pdf直接复制pom配置了","categories":[],"tags":[{"name":"踩坑","slug":"踩坑","permalink":"http://example.com/tags/%E8%B8%A9%E5%9D%91/"}]},{"title":"shell","slug":"shell","date":"2021-05-11T14:24:50.000Z","updated":"2021-11-02T06:06:24.247Z","comments":true,"path":"2021/05/11/shell/","link":"","permalink":"http://example.com/2021/05/11/shell/","excerpt":"Shell概述","text":"Shell概述 Shell解析器 Linux提供的Shell解析器有： 1234567[root@hadoop100 ~]# cat /etc/shells /bin/sh/bin/bash/usr/bin/sh/usr/bin/bash/bin/tcsh/bin/csh bash和sh的关系 12345[root@hadoop100 bin]# ll | grep bash-rwxr-xr-x. 1 root root 964536 Apr 1 2020 bashlrwxrwxrwx. 1 root root 10 Apr 29 17:16 bashbug -&gt; bashbug-64-rwxr-xr-x. 1 root root 6964 Apr 1 2020 bashbug-64lrwxrwxrwx. 1 root root 4 Apr 29 17:16 sh -&gt; bash Centos默认的解析器是bash 12[root@hadoop100 bin]# echo $SHELL/bin/bash Shell脚本入门 脚本格式 脚本以#!/bin/bash开头（指定解析器） 第一个Shell脚本：helloworld （1）需求：创建一个Shell脚本，输出helloworld （2）案例实操： 123456[root@hadoop100 ~]$ touch helloworld.sh[root@hadoop100 ~]$ vi helloworld.sh在helloworld.sh中输入如下内容#!/bin/bashecho &quot;helloworld&quot; （3）脚本的常用执行方式 第一种：采用bash或sh+脚本的相对路径或绝对路径（不用赋予脚本+x权限） sh+脚本的相对路径： 12[root@hadoop100 ~]$ sh helloworld.sh Helloworld sh+脚本的绝对路径： 12[root@hadoop100 ~]$ sh /home/atguigu/datas/helloworld.sh helloworld bash+脚本的相对路径 bash+脚本的绝对路径 第二种：采用输入脚本的绝对路径或相对路径执行脚本（必须具有可执行权限+x） 首先要赋予helloworld.sh 脚本的+x权限 1[root@hadoop100 ~]$ chmod +x helloworld.sh 执行脚本 相对路径： 12[root@hadoop100 ~]$ ./helloworld.sh Helloworld 绝对路径： 12[root@hadoop100 ~]$ /home/atguigu/datas/helloworld.sh Helloworld 注意：第一种执行方法，本质是bash解析器帮你执行脚本，所以脚本本身不需要执行权限。第二种执行方法，本质是脚本需要自己执行，所以需要执行权限。 第二个Shell脚本：多命令处理 （1）需求： 在/root目录下创建一个banzhang.txt,在banzhang.txt文件中增加“I love cls”。 （2）案例实操： 123456789[root@hadoop100 ~]$ touch batch.sh[root@hadoop100 ~]$ vi batch.sh在batch.sh中输入如下内容#!/bin/bashcd /roottouch cls.txtecho &quot;I love cls&quot; &gt;&gt; cls.txt Shell中的变量系统变量 常用系统变量 $HOME、​$PWD、$SHELL、​$USER等 案例实操 （1）查看系统变量的值 12[root@hadoop100 ~]# echo $HOME/root 自定义变量 基本语法 （1）定义变量：变量=值 （2）撤销变量：unset 变量 （3）声明静态变量：readonly变量，注意：不能unset 变量定义规则 （1）变量名称可以由字母、数字和下划线组成，但是不能以数字开头，环境变量名建议大写。 （2）等号两侧不能有空格 （3）在bash中，变量默认类型都是字符串类型，无法直接进行数值运算。 （4）变量的值如果有空格，需要使用双引号或单引号括起来 案例实操 （1）定义变量A 123[root@hadoop100 ~]# A=5[root@hadoop100 ~]# echo $A5 （2）给变量A重新赋值 123[root@hadoop100 ~]# A=8[root@hadoop100 ~]# echo $A8 （3）撤销变量A 123[root@hadoop100 ~]# unset A[root@hadoop100 ~]# echo $A （4）声明静态的变量B=2，不能unset 12345[root@hadoop100 ~]# readonly B=2[root@hadoop100 ~]# echo $B2[root@hadoop100 ~]# B=9-bash: B: readonly variable （5）在bash中，变量默认类型都是字符串类型，无法直接进行数值运算 123[root@hadoop100 ~]# C=1+2[root@hadoop100 ~]# echo $C1+2 （6）变量的值如果有空格，需要使用双引号或单引号括起来 12345[root@hadoop100 ~]# D=hello worldbash: world: command not found...[root@hadoop100 ~]# D=&quot;hello world&quot;[root@hadoop100 ~]# echo $Dhello world （7）可把变量提升为全局环境变量，可供其他Shell程序使用 export 变量名 12345678910[root@hadoop100 ~]# vim hello.sh在hello.sh文件中增加echo $B#!/bin/bashecho &quot;hello world!&quot;echo $B[root@hadoop100 ~]# ./hello.sh hello world! 发现并没有打印输出变量B的值。 1234[root@hadoop100 ~]# export B[root@hadoop100 ~]# ./hello.sh hello world!2 特殊变量：$n 基本语法 $n （功能描述：n为数字，​$0代表该脚本名称，​$1-​$9代表第一到第九个参数，十以上的参数，十以上的参数需要用大括号包含，${10}） 案例实操 （1）输出该脚本文件名称、输入参数1和输入参数2 的值 1234567[root@hadoop100 ~]# vim parameter.sh#!/bin/bashecho &quot;$0 $1 $2&quot;[root@hadoop100 ~]# bash parameter.sh aa bbparameter.sh aa bb 特殊变量：$ 基本语法 $# （功能描述：获取所有输入参数个数，常用于循环）。 案例实操 （1）获取输入参数的个数 123456789[root@hadoop100 ~]# vim parameter.sh#!/bin/bashecho &quot;$0 $1 $2&quot;echo $#[root@hadoop100 ~]# bash parameter.sh aa bbparameter.sh aa bb2 特殊变量：$*、$@ 基本语法 $* （功能描述：这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体） $@ （功能描述：这个变量也代表命令行中所有的参数，不过$@把每个参数区分对待） 案例实操 （1）打印输入的所有参数 12345678910111213[root@hadoop100 ~]# vim parameter.sh#!/bin/bashecho &quot;$0 $1 $2&quot;echo $#echo $*echo $@[root@hadoop100 ~]# bash parameter.sh aa bb ccparameter.sh aa bb3aa bb ccaa bb cc 特殊变量：$？ 基本语法 $？ （功能描述：最后一次执行的命令的返回状态。如果这个变量的值为0，证明上一个命令正确执行；如果这个变量的值为非0（具体是哪个数，由命令自己来决定），则证明上一个命令执行不正确了。） 案例实操 （1）判断helloworld.sh脚本是否正确执行 12345[root@hadoop100 ~]# bash hello.sh hello world!2[root@hadoop100 ~]# echo $?0 运算符 基本语法 （1）“$((运算式))”或“$[运算式]” （2）expr + , - , *, /, % 加，减，乘，除，取余 ​ 注意：expr运算符间要有空格 案例实操： （1）计算3+2的值 123456[root@hadoop100 ~]# echo $[3 + 2]5[root@hadoop100 ~]# echo $[ 3 +2]5[root@hadoop100 ~]# expr 2 + 35 （2）计算（2 + 3）* 4的值 1234567# expr一步完成计算[root@hadoop100 ~]# expr `expr 2 + 3` \\* 420# 采用$[运算式]方式[root@hadoop100 ~]# echo $(((2 + 3) * 4))20 条件判断 条件判断 [ condition ] （注意condition前后要有空格） 注意：条件非空即为true，[ condition ]返回true，[] 返回false。 常用判断条件 （1）两个整数之间比较 ​ = 字符串比较 ​ -lt 小于（less than） -le 小于等于（less equal） ​ -eq 等于（equal） -gt 大于（greater than） ​ -ge 大于等于（greater equal） -ne 不等于（Not equal） （2）按照文件权限进行判断 ​ -r 有读的权限（read） -w 有写的权限（write） ​ -x 有执行的权限（execute） （3）按照文件类型进行判断 ​ -f 文件存在并且是一个常规的文件（file） ​ -e 文件存在（existence） -d 文件存在并是一个目录（directory） 案例实操 （1）23是否大于等于22 123[root@hadoop100 ~]# [ 22 -lt 23 ][root@hadoop100 ~]# echo $?0 （2）hello.sh是否具有写权限 123[root@hadoop100 ~]# [ -w hello.sh ][root@hadoop100 ~]# echo $?0 （3）/root/hello.sh目录中的文件是否存在 123456[root@hadoop100 ~]# [ -e /root/hello.sh ][root@hadoop100 ~]# echo $?0[root@hadoop100 ~]# [ -e /root/hello1.sh ][root@hadoop100 ~]# echo $?1 （4）多条件判断（&amp;&amp; 表示前一条命令执行成功时，才执行后一条命令，|| 表示上一条命令执行失败后，才执行下一条命令） 1234[root@hadoop100 ~]# [ -e /root/hello.sh ] &amp;&amp; echo &quot;hello.sh存在&quot;hello.sh存在[root@hadoop100 ~]# [ -e /root/a.sh ] || echo &quot;a.sh不存在&quot;a.sh不存在 流程控制（重点）if 判断 基本语法 1234567891011121314if [ 条件判断式 ]then 程序 fi 或者 if [ 条件判断式 ] then 程序 elif [ 条件判断式 ] then 程序else 程序fi 注意事项： （1）[ 条件判断式 ]，中括号和条件判断式之间必须有空格 （2）if后要有空格 案例实操 （1）输入一个数字，如果是1，则输出aaa，如果是2，则输出ccc，如果是其它，什么也不输出。 123456789101112131415[root@hadoop100 ~]# vim if.sh#!/bash/binif [ $1 = 1 ] then echo &quot;aaa&quot;elif [ $1 = 2 ] then echo &quot;bbb&quot;fi[root@hadoop100 ~]# bash if.sh 1aaa[root@hadoop100 ~]# bash if.sh 2bbb case 语句 基本语法 123456789101112case $变量名 in &quot;值1&quot;） 如果变量的值等于值1，则执行程序1 ;; &quot;值2&quot;） 如果变量的值等于值2，则执行程序2 ;; …省略其他分支… *） 如果变量的值都不是以上的值，则执行此程序 ;; esac 注意事项： （1) case行尾必须为单词“in”，每一个模式匹配必须以右括号“）”结束。 （2) 双分号“;;”表示命令序列结束，相当于java中的break。 （3) 最后的“*）”表示默认模式，相当于java中的default，*不可以加双引号。 案例实操 （1）输入一个数字，如果是1，则输出aaa，如果是2，则输出bbb，如果是其它，输出ccc。 123456789101112131415161718192021[root@hadoop100 ~]# vim case.sh#!/bin/bashcase $1 in&quot;1&quot;) echo aaa;;&quot;2&quot;) echo bbb;;*) echo ccc;;esac[root@hadoop100 ~]# bash case.sh ccc[root@hadoop100 ~]# bash case.sh 1aaa[root@hadoop100 ~]# bash case.sh 12ccc for 循环 基本语法1 1234for ((初始值;循环控制条件;变量变化)) do 程序 done 案例实操 （1）从1加到100 123456789101112[root@hadoop100 ~]# vim for.sh#!/bin/bashsum=0for((i=1;i&lt;=$1;i++))do sum=$[$sum + $i]doneecho $sum[root@hadoop100 ~]# bash for.sh 1005050 基本语法2 1234for 变量 in 值1 值2 值3… do 程序 done 案例实操 （1）打印所有输入参数 123456789101112131415[root@hadoop100 ~]# vim for2.sh #!/bash/binfor i in $* do echo $i done[root@hadoop100 ~]# bash for2.sh 1 2 3 4 5 6123456 （2）比较$*和$@区别 ​ （a）$*和​$@都表示传递给函数或脚本的所有参数，不被双引号“”包含时，都以$1 $2 …​$n的形式输出所有参数。 ​ （b）当它们被双引号“”包含时，“$*”会将所有的参数作为一个整体，以“​$1 ​$2 …$n”的形式输出所有参数；“$@”会将各个参数分开，以“​$1” “​$2”…”​$n”的形式输出所有参数。 12345678910111213141516171819202122232425262728293031323334353637[root@hadoop100 ~]# vim for.sh #!/bin/bashfor i in $* do echo &#x27;$*:&#x27;$i doneecho &quot;==========&quot;for i in $@ do echo &#x27;$@:&#x27;$i doneecho &quot;==========&quot;for i in &quot;$*&quot; do echo &#x27;&quot;$*&quot;:&#x27;$i doneecho &quot;==========&quot;for i in &quot;$@&quot; do echo &#x27;&quot;$@&quot;:&#x27;$i done[root@hadoop100 ~]# bash for.sh 1 2 3$*:1$*:2$*:3==========$@:1$@:2$@:3==========&quot;$*&quot;:1 2 3==========&quot;$@&quot;:1&quot;$@&quot;:2&quot;$@&quot;:3 while 循环 基本语法 1234while [ 条件判断式 ] do 程序 done 案例实操 （1）从1加到100 1234567891011121314[root@hadoop100 ~]# vim while.sh#!/bash/bini=1sum=0while [ $i -le 100 ] do sum=$(($sum + $i)) i=$[$i+1] doneecho $sum[root@hadoop100 ~]# bash while.sh 5050 注意事项： while后面需要有空格 read读取控制台输入 基本语法 ​ read(选项)(参数) ​ 选项： ​ -p：指定读取值时的提示符； ​ -t：指定读取值时等待的时间（秒）。 ​ 参数 变量：指定读取值的变量名 案例实操 （1）提示7秒内，读取控制台输入的名称 1234[root@hadoop100 ~]# read -t 5 -p &quot;请输入用户名:&quot; name请输入用户名:zhangsan[root@hadoop100 ~]# echo $namezhangsan 函数系统函数 basename基本语法 basename [string / pathname] [suffix] （功能描述：basename命令会删掉所有的前缀包括最后一个（‘/’）字符，然后将字符串显示出来。 选项： suffix为后缀，如果suffix被指定了，basename会将pathname或string中的suffix去掉。 案例实操 （1）截取该/root/hello.sh路径的文件名称 1234[root@hadoop100 ~]# basename /root/hello.sh hello.sh[root@hadoop100 ~]# basename /root/hello.sh .shhello dirname基本语法 dirname 文件绝对路径 （功能描述：从给定的包含绝对路径的文件名中去除文件名（非目录的部分），然后返回剩下的路径（目录的部分）） 案例实操 （1）获取hello.sh文件的路径 12[root@hadoop100 ~]# dirname /root/hello.sh /root 自定义函数 基本语法 123456[ function ] funname[()]&#123; Action; [return int;]&#125;funname 经验技巧 （1）必须在调用函数地方之前，先声明函数，shell脚本是逐行运行。不会像其它语言一样先编译。 （2）函数返回值，只能通过$?系统变量获得，可以显示加return返回，如果不加，将以最后一条命令运行结果，作为返回值。return后跟数值n(0-255) 案例实操 （1）计算两个输入参数的和 123456789101112131415161718[root@hadoop100 ~]# vim fun.sh#!/bin/bashfunction sum()&#123; s=0 s=$[ $1 + $2 ] echo &quot;$s&quot;&#125;read -p &quot;Please input the number1: &quot; n1;read -p &quot;Please input the number2: &quot; n2;sum $n1 $n2;[root@hadoop100 ~]# bash fun.sh Please input the number1: 5Please input the number2: 611 Shell工具（重点）cutcut的工作就是“剪”，具体的说就是在文件中负责剪切数据用的。cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段输出。 基本用法 cut [选项参数] filename 说明：默认分隔符是制表符 选项参数说明 选项参数 功能 -f 列号，提取第几列 -d 分隔符，按照指定分隔符分割列 -c 指定具体的字符 案例实操 （0）数据准备 1234567[root@hadoop100 ~]# head -n 5 /etc/passwd &gt; p.txt [root@hadoop100 ~]# cat p.txt root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin （1）切割p.txt第一个字符 123456[root@hadoop100 ~]# cut -c 1 p.txt rbdal （2）切割p.txt第一、三列 123456[root@hadoop100 ~]# cut -d : -f 1,3 p.txt root:0bin:1daemon:2adm:3lp:4 （3）查找root行 再切割 12[root@hadoop100 ~]# cat p.txt | grep root | cut -d : -f 2-3 x:0 sedsed是一种流编辑器，它一次处理一行内容。处理时，把当前处理的行存储在临时缓冲区中，称为“模式空间”，接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。文件内容并没有改变，除非你使用重定向存储输出。 基本用法 sed [选项参数] ‘command’ filename 选项参数说明 选项参数 功能 -e 直接在指令列模式上进行sed的动作编辑。 -i 直接编辑文件 命令功能描述 命令 功能描述 a 新增，a的后面可以接字串，在下一行出现 d 删除 s 查找并替换 案例实操 （1）将“hello world!”这个单词插入到p.txt第二行下，打印。 12345678910111213[root@hadoop100 ~]# sed &#x27;2ahello world!&#x27; p.txtroot:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologinhello world!daemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin[root@hadoop100 ~]# cat p.txt root:x:0:0:root:/root:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 注意：文件并没有改变 （2）删除p.txt文件所有包含root的行 12345[root@hadoop100 ~]# sed &#x27;/root/d&#x27; p.txt bin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin （3）将p.txt文件中root替换为toor 123456[root@hadoop100 ~]# sed &#x27;s/root/toor/g&#x27; p.txt toor:x:0:0:toor:/toor:/bin/bashbin:x:1:1:bin:/bin:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologi 注意：‘g’表示global，全部替换 （4）将p.txt文件中的第二行删除并将root替换为xxxx 12345[root@hadoop100 ~]# sed -e &#x27;2d&#x27; -e &#x27;s/root/xxxx/g&#x27; p.txt xxxx:x:0:0:xxxx:/xxxx:/bin/bashdaemon:x:2:2:daemon:/sbin:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologinlp:x:4:7:lp:/var/spool/lpd:/sbin/nologin awk一个强大的文本分析工具，把文件逐行的读入，以空格为默认分隔符将每行切片，切开的部分再进行分析处理。 基本用法 awk [选项参数] ‘pattern1{action1} pattern2{action2}…’ filename pattern：表示AWK在数据中查找的内容，就是匹配模式 action：在找到匹配内容时所执行的一系列命令 选项参数说明 选项参数 功能 -F 指定输入文件折分隔符 -v 赋值一个用户定义变量 案例实操 （1）搜索p.txt文件以root关键字开头的所有行，并输出该行的第7列。 12[root@hadoop100 ~]# awk -F : &#x27;/^root/&#123;print $7&#125;&#x27; p.txt /bin/bash （2）搜索p.txt文件以root关键字开头的所有行，并输出该行的第1列和第7列，中间以“，”号分割。 12[root@hadoop100 ~]# awk -F: &#x27;/^root/&#123;print $1&quot;,&quot;$7&#125;&#x27; p.txt root,/bin/bash 注意：只有匹配了pattern的行才会执行action （3）只显示p.txt的第一列和第七列，以逗号分割，且在所有行前面添加”the begin of the world”在最后一行添加”the end of the world”。 12345678[root@hadoop100 ~]# awk -F : &#x27;BEGIN&#123;print &quot;the begin of the world&quot;&#125; &#123;print $1&quot;,&quot;$7&#125; END&#123;print &quot;the end of the world&quot;&#125;&#x27; p.txt the begin of the worldroot,/bin/bashbin,/sbin/nologindaemon,/sbin/nologinadm,/sbin/nologinlp,/sbin/nologinthe end of the world 注意：BEGIN 在所有数据读取行之前执行；END 在所有数据执行之后执行。 （4）将p.txt文件中的用户id增加数值10并输出 123456[root@hadoop100 ~]# awk -F : -v i=10 &#x27;&#123;print $3+i&#125;&#x27; p.txt 1011121314 awk的内置变量 变量 说明 FILENAME 文件名 NR 已读的记录数 NF 浏览记录的域的个数（切割后，列的个数） 案例实操 （1）统计passwd文件名，每行的行号，每行的列数 123456[root@hadoop100 ~]# awk -F : &#x27;&#123;print &quot;filename:&quot; FILENAME &quot;, linenumber:&quot; NR &quot;, columns:&quot; NF&#125;&#x27; p.txt filename:p.txt, linenumber:1, columns:7filename:p.txt, linenumber:2, columns:7filename:p.txt, linenumber:3, columns:7filename:p.txt, linenumber:4, columns:7filename:p.txt, linenumber:5, columns:7 （2）查询p.txt中空行所在的行号 1[root@hadoop100 ~]# awk &#x27;/^$/&#123;print NR&#125;&#x27; p.txt sortsort命令是在Linux里非常有用，它将文件进行排序，并将排序结果标准输出。 基本语法 sort(选项)(参数) | 选项 | 说明 || —— | ———————————— || -n | 依照数值的大小排序 || -r | 以相反的顺序来排序 || -t | 设置排序时所用的分隔字符 || -k | 指定需要排序的列 | 参数：指定待排序的文件列表 案例实操 （1）按照“：”分割后的第三列倒序排序。 123456[root@hadoop100 ~]# sort -t : -nrk 3 p.txt lp:x:4:7:lp:/var/spool/lpd:/sbin/nologinadm:x:3:4:adm:/var/adm:/sbin/nologindaemon:x:2:2:daemon:/sbin:/sbin/nologinbin:x:1:1:bin:/bin:/sbin/nologinroot:x:0:0:root:/root:/bin/bash","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"shell","slug":"shell","permalink":"http://example.com/tags/shell/"}]},{"title":"linux下mysql安装","slug":"linux下mysql安装","date":"2021-05-10T05:08:04.000Z","updated":"2021-10-19T11:02:09.553Z","comments":true,"path":"2021/05/10/linux下mysql安装/","link":"","permalink":"http://example.com/2021/05/10/linux%E4%B8%8Bmysql%E5%AE%89%E8%A3%85/","excerpt":"检查当前系统是否安装过MysqlCentOS 6命令：rpm -qa | grep mysql 默认Linux在安装的时候，自带了mysql相关的组件。 先卸载系统自带的mysql，执行卸载命令 rpm -e --nodeps mysql-libs CentOS 7命令：rpm -qa | grep mariadb 不检查依赖卸载 rpm -e --nodeps mariadb-libs 检查/tmp目录的权限是否是满的","text":"检查当前系统是否安装过MysqlCentOS 6命令：rpm -qa | grep mysql 默认Linux在安装的时候，自带了mysql相关的组件。 先卸载系统自带的mysql，执行卸载命令 rpm -e --nodeps mysql-libs CentOS 7命令：rpm -qa | grep mariadb 不检查依赖卸载 rpm -e --nodeps mariadb-libs 检查/tmp目录的权限是否是满的 Mysql的安装安装的版本是mysql 5.7（最新版本安装过程与5.7.16有差异） 通过Xft文件传输工具将rpm安装包传输到opt目录下执行rpm安装，必须按照下面的顺序安装12345[root@hadoop100 ~]# rpm -ivh mysql-community-common-5.7.16-1.el7.x86_64.rpm[root@hadoop100 ~]# rpm -ivh mysql-community-libs-5.7.16-1.el7.x86_64.rpm[root@hadoop100 ~]# rpm -ivh mysql-community-libs-compat-5.7.16-1.el7.x86_64.rpm[root@hadoop100 ~]# rpm -ivh mysql-community-client-5.7.16-1.el7.x86_64.rpm[root@hadoop100 ~]# rpm -ivh mysql-community-server-5.7.16-1.el7.x86_64.rpm 查看是否安装成功：mysqladmin --version 或者也可以通过rpm命令来查看 rpm -qa | grep mysqlmysql服务的初始化为了保证数据库目录为与文件的所有者为 mysql 登录用户，如果你是以 root 身份运行 mysql 服务，需要执行下面的命令初始化 mysqld --initialize --user=mysql 另外 —initialize 选项默认以“安全”模式来初始化，则会为 root 用户生成一个密码并将该密码标记为过期，登录后你需要设置一个新的密码 查看密码：cat /var/log/mysqld.log root@localhost: 后面就是初始化的密码 启动MySQL的服务 1[root@hadoop100 ~]# systemctl start mysqld 更新密码 首次登陆通过 mysql -uroot -p进行登录，在Enter password：录入初始化密码 因为初始化密码默认是过期的，所以查看数据库会报错 修改密码： 1ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;你的密码&#39;; 但新密码太简单会报错，设置完密码就可以用新密码登录，正常使用数据库了 Mysql服务Mysql服务自启状态查看是否是开机自启：systemctl is-enabled mysqld (默认是开机自启的) 查看启动状态：systemctl status mysqld 启动之后，查看进程：ps -ef | grep mysql Mysql的安装位置 参数 路径 解释 备注 —datadir /var/lib/mysql/ mysql数据库文件的存放路径 —basedir /usr/bin 相关命令目录 mysqladmin mysqldump等命令 —plugin-dir /usr/lib64/mysql/plugin mysql插件存放路径 —log-error /var/lib/mysql/jack.atguigu.err mysql错误日志路径 —pid-file /var/lib/mysql/jack.atguigu.pid 进程pid文件 —socket /var/lib/mysql/mysql.sock 本地连接时用的unix套接字文件 /usr/share/mysql 配置文件目录 mysql脚本及配置文件 /etc/init.d/mysql 服务启停相关脚本 Mysql服务的自启动1[root@hadoop100 ~]# systemctl enable mysqld 修改字符集常用命令 SQL语句 描述 备注 show databases 列出所有数据库 create database 库名 创建一个数据库 create database 库名 character set utf8 创建数据库，顺便执行字符集为utf-8 show create database 库名 查看数据库的字符集 show variables like ‘%char%’ 查询所有跟字符集相关的信息 set [字符集属性]=utf8 设置相应的属性为utf8 只是临时修改，当前有效。服务重启后，失效。 alter database 库名character set ‘utf8’ 修改数据库的字符集 alter table 表名convert to character set ‘utf8’ 修改表的字符集 实验SQL CREATE database mydb; CREATE table mytable(id int,name varchar(30)); insert into mytable(id,name) values (1,’jack’); insert into mytable(id,name) values (2,’张三’) 直接插入中文数据报错原因如果在建库建表的时候，没有明确指定字符集，则采用默认的字符集latin1,其中是不包含中文字符的。查看默认的编码字符集： 1234567891011121314mysql&gt; show variables like &#39;%char%&#39;;+--------------------------+----------------------------+| Variable_name | Value |+--------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | utf8 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | utf8 || character_set_system | utf8 || character_sets_dir | &#x2F;usr&#x2F;share&#x2F;mysql&#x2F;charsets&#x2F; |+--------------------------+----------------------------+8 rows in set (0.01 sec) 永久修改修改配置文件： vim /etc/my.cnf 在mysqld节点下最后加上中文字符集配置 character_set_server=utf8 重新启动mysql服务 注意：已经创建的数据库的设定不会发生变化，参数修改只对新建的数据库有效！ 已生成的库表字符集如何变更修改数据库的字符集 ​ alter database 数据库名 character set ‘utf8’; 修改数据表的字符集 ​ alter table 表名 convert to character set ‘utf8’; 让root用户可以远程连接1mysql&gt; update user set host&#x3D;&quot;%&quot; where user&#x3D;&#39;root&#39;;","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"}]},{"title":"linux软件包管理","slug":"linux软件包管理","date":"2021-05-09T07:45:25.000Z","updated":"2021-05-18T13:31:01.758Z","comments":true,"path":"2021/05/09/linux软件包管理/","link":"","permalink":"http://example.com/2021/05/09/linux%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%AE%A1%E7%90%86/","excerpt":"RPMRPM概述RPM（RedHat Package Manager），RedHat软件包管理工具，类似windows里面的setup.exe 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。 RPM包的名称格式 Apache-1.3.23-11.i386.rpm “apache” 软件名称 “1.3.23-11”软件的版本号，主版本和此版本 “i386”是软件所运行的硬件平台，Intel 32位微处理器的统称 “rpm”文件扩展名，代表RPM包","text":"RPMRPM概述RPM（RedHat Package Manager），RedHat软件包管理工具，类似windows里面的setup.exe 是Linux这系列操作系统里面的打包安装工具，它虽然是RedHat的标志，但理念是通用的。 RPM包的名称格式 Apache-1.3.23-11.i386.rpm “apache” 软件名称 “1.3.23-11”软件的版本号，主版本和此版本 “i386”是软件所运行的硬件平台，Intel 32位微处理器的统称 “rpm”文件扩展名，代表RPM包 RPM查询命令（rpm -qa） 基本语法 rpm -qa （功能描述：查询所安装的所有rpm软件包） 经验技巧 由于软件包比较多，一般都会采取过滤。rpm -qa | grep rpm软件包 案例实操 查询firefox软件安装情况 12[root@hadoop100 ~]# rpm -qa | grep firefoxfirefox-68.10.0-1.el7.centos.x86_64 RPM卸载命令（rpm -e） 基本语法 （1）rpm -e RPM软件包 （2） rpm -e —nodeps 软件包 选项说明 选项 功能 -e 卸载软件包 —nodeps 卸载软件时，不检查依赖。这样的话，那些使用该软件包的软件在此之后可能就不能正常工作了。 案例实操 卸载firefox软件 1[root@hadoop100 ~]# rpm -e firefox RPM安装命令（rpm -ivh） 基本语法 rpm -ivh RPM包全名 选项说明 选项 功能 -i -i=install，安装 -v -v=verbose，显示详细信息 -h -h=hash，进度条 —nodeps —nodeps，不检测依赖进度 案例实操 安装firefox软件 1[root@hadoop101 Packages]# rpm -ivh firefox-45.0.1-1.el6.centos.x86_64.rpm YUM仓库配置YUM概述YUM（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及CentOS中的Shell前端软件包管理器。基于RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装 YUM的常用命令 基本语法 yum [选项] [参数] 选项说明 选项 功能 -y 对所有提问都回答“yes” 参数说明 参数 功能 install 安装rpm软件包 update 更新rpm软件包 check-update 检查是否有可用的更新rpm软件包 remove 删除指定的rpm软件包 list 显示软件包信息 clean 清理yum过期的缓存 deplist 显示yum软件包的所有依赖关系 案例实操实操 采用yum方式安装firefox 1[root@hadoop100 ~]#yum -y install firefox.x86_64 修改网络YUM源默认的系统YUM源，需要连接国外apache网站，网速比较慢，可以修改关联的网络YUM源为国内镜像的网站，比如网易163。 前期文件准备 （1）前提条件linux系统必须可以联网 （2）在Linux环境中访问该网络地址：http://mirrors.163.com/.help/centos.html ，在使用说明中点击CentOS7 -&gt; 再点击保存 （3）查看文件保存的位置 替换本地yum文件 （1）把下载的文件移动到/etc/yum.repos.d/目录 1[root@hadoop100 Downloads]# mv CentOS7-Base-163.repo /etc/yum.repos.d/ （2）进入到/etc/yum.repos.d/目录 （3）用CentOS7-Base-163.repo替换CentOS-Base.repo 1[root@hadoop100 yum.repos.d]# mv CentOS7-Base-163.repo CentOS-Base.repo 安装命令 12[root@hadoop100 yum.repos.d]# yum clean all[root@hadoop100 yum.repos.d]# yum makecache 测试 123[root@hadoop100 yum.repos.d]# yum list | grep firefoxfirefox.x86_64 78.10.0-1.el7.centos @updates firefox.i686 78.10.0-1.el7.centos updates","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"linux常用基本命令","slug":"linux常用基本命令","date":"2021-05-07T08:37:37.000Z","updated":"2021-11-05T08:55:37.882Z","comments":true,"path":"2021/05/07/linux常用基本命令/","link":"","permalink":"http://example.com/2021/05/07/linux%E5%B8%B8%E7%94%A8%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/","excerpt":"帮助命令man 获得帮助信息 基本语法 1man [命令或配置文件] 显示说明 信息 功能 NAME 命令的名称和单行描述 SYNOPSIS 怎样使用命令 DESCRIPTION 命令功能的深入讨论 EXAMPLES 怎样使用命令的例子 SEE ALSO 相关主题（通常是手册页）","text":"帮助命令man 获得帮助信息 基本语法 1man [命令或配置文件] 显示说明 信息 功能 NAME 命令的名称和单行描述 SYNOPSIS 怎样使用命令 DESCRIPTION 命令功能的深入讨论 EXAMPLES 怎样使用命令的例子 SEE ALSO 相关主题（通常是手册页） 案例实操 查看ls命令的帮助信息 1[root@hadoop100 ~]# man ls help 获得shell内置命令的帮助信息 基本语法 1help 命令 案例实操 查看cd命令的帮助信息 1[root@hadoop100 ~]# help cd 常用快捷键 常用快捷键 功能 ctrl + c 停止进程 ctrl + l reset ctrl + q 退出 文件目录类pwd 显示当前工作目录的绝对路径ls:list 列出目录内容 基本语法 1pwd 案例实操 显示当前工作目录的绝对路径 12[root@hadoop100 ~]# pwd/root ls 列出目录的内容 基本语法 1ls [选项] [目录或是文件] 选项说明 选项 功能 -a 全部的文件，连同隐藏档( 开头为 . 的文件) 一起列出来(常用) -l 长数据串列出，包含文件的属性与权限等等数据；(常用) 显示说明 每行列出的信息依次是：文件类型与权限 链接数 文件属主 文件属组 文件大小用byte来表示 建立或最近修改的时间 名字 案例实操 查看当前目录的所有内容信息 1234567[root@hadoop100 ~]# ls -altotal 60dr-xr-x---. 15 root root 4096 May 7 15:45 .dr-xr-xr-x. 17 root root 224 Apr 29 17:23 ..-rw-------. 1 root root 1533 Apr 29 17:24 anaconda-ks.cfg-rw-------. 1 root root 1057 Apr 30 16:41 .bash_history-rw-r--r--. 1 root root 18 Dec 29 2013 .bash_logout cd 切换目录cd:Change Directory切换路径 基本语法 1cd [参数] 参数说明 参数 功能 cd 绝对路径 切换路径 cd 相对路径 切换路径 cd ~ 回到自己的家目录 cd - 回到上一次所在目录 cd .. 回到当前目录的上一级目录 cd -P 跳转到实际物理路径，而非快捷方式路径 案例实操 使用绝对路径切换到root目录 1[root@hadoop101 ~]# cd /root/ 使用相对路径切换到“公共的”目录 12[root@hadoop101 ~]# cd Public/[root@hadoop100 Public]# 表示回到自己的家目录，亦即是 /root 这个目录 12[root@hadoop100 Public]# cd ~[root@hadoop100 ~]# cd- 回到上一次所在目录 123[root@hadoop100 ~]# cd -/root/Public[root@hadoop100 Public]# 表示回到当前目录的上一级目录，亦即是 “/root/Public”的上一级目录的意思 12[root@hadoop100 Public]# cd ..[root@hadoop100 ~]# mkdir 创建一个新的目录mkdir:Make directory 建立目录 基本语法 1mkdir [选项] 要创建的目录 选项说明 选项 功能 -p 创建多层目录 案例实操 创建一个目录 12[root@hadoop100 ~]# mkdir test[root@hadoop100 ~]# mkdir test/test 创建一个多级目录 1[root@hadoop100 ~]# mkdir -p a/b/c rmdir 删除一个空的目录rmdir:Remove directory 移动目录 基本语法 1rmdir 要删除的空目录 案例实操 删除一个空的文件夹 1[root@hadoop100 ~]# rmdir a/b/c test/test/ #删除掉c和第二级的test touch 创建空文件 基本语法 1touch 文件名称 案例实操 1[root@hadoop100 ~]# touch a.txt cp 复制文件或目录 基本语法 1cp [选项] source dest （功能描述：复制source文件到dest） 选项说明 选项 功能 -r 递归复制整个文件夹 参数说明 参数 功能 source 源文件 dest 目标文件 经验技巧 强制覆盖不提示的方法：\\cp 案例实操 复制文件 1[root@hadoop100 ~]# cp a.txt test/test 递归复制整个文件夹 1[root@hadoop100 ~]# cp -r test/test a/b/c rm 移除文件或目录 基本语法 1rm [选项] deleteFile （功能描述：递归删除目录中所有内容） 选项说明 选项 功能 -r 递归删除目录中所有内容 -f 强制执行删除操作，而不提示用于进行确认。 -v 显示指令的详细执行过程 案例实操 删除目录中的内容 1[root@hadoop100 ~]# rm a.txt 递归删除目录中所有内容 1[root@hadoop100 ~]# rm -rf a/ mv 移动文件与目录或重命名 基本语法 mv oldNameFile newNameFile （功能描述：重命名） mv /temp/movefile /targetFolder （功能描述：移动文件） 案例实操 重命名 1[root@hadoop100 ~]# mv a/b/c/a.txt a/b/c/a.xxx 移动文件 1[root@hadoop100 ~]# mv a/b/c/a.txt a/b/ cat 查看文件内容查看文件内容，从第一行开始显示。 基本语法 1cat [选项] 要查看的文件 选项说明 选项 功能描述 -n 显示所有行的行号，包括空行。 经验技巧 一般查看比较小的文件，一屏幕能显示全的。 案例实操 查看文件内容并显示行号 1[root@hadoop100 ~]# cat -n a/b/a.txt more 文件内容分屏查看器more指令是一个基于VI编辑器的文本过滤器，它以全屏幕的方式按页显示文本文件的内容。more指令中内置了若干快捷键，详见操作说明。 基本语法 more 要查看的文件 操作说明 操作 功能说明 空格键 (space) 代表向下翻一页 Enter 代表向下翻『一行』 q 代表立刻离开 more ，不再显示该文件内容。 Ctrl+F 向下滚动一屏 Ctrl+B 返回上一屏 = 输出当前行的行号 :f 输出文件名和当前行的行号 案例实操 采用more查看文件 1[root@hadoop100 ~]# more anaconda-ks.cfg less 分屏显示文件内容less指令用来分屏查看文件内容，它的功能与more指令类似，但是比more指令更加强大，支持各种显示终端。less指令在显示文件内容时，并不是一次将整个文件加载之后才显示，而是根据显示需要加载内容，对于显示大型文件具有较高的效率。 基本语法 less 要查看的文件 操作说明 操作 功能说明 空格键 向下翻动一页 [pagedown] 向下翻动一页 [pageup] 向上翻动一页 /字串 向下搜寻『字串』的功能；n：向下查找；N：向上查找； ?字串 向上搜寻『字串』的功能；n：向上查找；N：向下查找； q 离开 less 这个程序； 案例实操 采用less查看文件 1[root@hadoop100 ~]# less anaconda-ks.cfg echoecho输出内容到控制台 基本语法 1echo [选项] [输出内容] 选项： ​ -e： 支持反斜线控制的字符转换 控制字符 作用 \\ 输出\\本身 \\n 换行符 \\t 制表符，也就是Tab键 案例实操 123[root@hadoop100 ~]# echo -e &quot;hello\\nworld&quot;helloworld head 显示文件头部内容head用于显示文件的开头部分内容，默认情况下head指令显示文件的前10行内容。 基本语法 head 文件 （功能描述：查看文件头10行内容） head -n 5 文件 （功能描述：查看文件头5行内容，5可以是任意行数） 选项说明 选项 功能 -n &lt;行数&gt; 指定显示头部内容的行数 案例实操 查看文件的头2行 1[root@hadoop100 ~]# head -n 8 anaconda-ks.cfg tail 输出文件尾部内容tail用于输出文件中尾部的内容，默认情况下tail指令显示文件的后10行内容。 基本语法 tail 文件 （功能描述：查看文件后10行内容） tail -n 5 文件 （功能描述：查看文件后5行内容，5可以是任意行数） tail -f 文件 （功能描述：实时追踪该文档的所有更新） 选项说明 选项 功能 -n&lt;行数&gt; 输出文件尾部n行内容 -f 显示文件最新追加的内容，监视文件变化 案例实操 查看文件后1行内容 1[root@hadoop100 ~]# tail -n 1 a/b/a.txt 实时追踪该档的所有更新 1[root@hadoop100 ~]# tail -f a/b/a.txt > 覆盖 和 &gt;&gt; 追加 基本语法 ll &gt;文件 （功能描述：列表的内容写入文件a.txt中（覆盖写）） ll &gt;&gt;文件 （功能描述：列表的内容追加到文件aa.txt的末尾） cat 文件1 &gt; 文件2 （功能描述：将文件1的内容覆盖到文件2） echo “内容” &gt;&gt; 文件 案例实操 将ls查看信息写入到文件中 1[root@hadoop100 ~]# ls &gt; ls.txt 将ls查看信息追加到文件中 1[root@hadoop100 ~]# ls -a &gt;&gt; ls.txt 采用echo将hello单词追加到文件中 1[root@hadoop100 ~]# echo hello &gt;&gt; ls.txt ln 软链接软链接也称为符号链接，类似于windows里的快捷方式，有自己的数据块，主要存放了链接其他文件的路径。 基本语法 ln -s [原文件或目录] [软链接名] （功能描述：给原文件创建一个软链接） 经验技巧 删除软链接： rm -rf 软链接名，而不是rm -rf 软链接名/ 查询：通过ll就可以查看，列表属性第1位是l，尾部会有位置指向。 案例实操 创建软连接 1234[root@hadoop100 ~]# mv ls.txt a/b/[root@hadoop100 ~]# ln -s a/b/ls.txt test0[root@hadoop100 ~]# lllrwxrwxrwx. 1 root root 10 May 7 18:36 test0 -&gt; a/b/ls.txt 删除软连接 1[root@hadoop100 ~]# rm -rf test0 进入软连接实际物理路径 123[root@hadoop100 ~]# ln -s a/b/ c[root@hadoop100 ~]# cd -P c[root@hadoop100 b]# history 查看已经执行过历史命令 基本语法 1history （功能描述：查看已经执行过历史命令） 实操实例 查看已经执行过的历史命令 1[root@hadoop100 ~]# history 用户管理命令useradd 添加新用户 基本语法 useradd 用户名 （功能描述：添加新用户） useradd -g 组名 用户名 （功能描述：添加新用户到某个组） 案例实操 添加一个用户 123[root@hadoop100 bin]# useradd zhangsan[root@hadoop100 bin]# ll /home/drwx------. 5 zhangsan zhangsan 107 May 7 20:22 zhangsan passwd 设置用户密码 基本语法 passwd 用户名 （功能描述：设置用户密码） 案例实操 设置用户的密码 1[root@hadoop100 bin]# passwd zhangsan id 查看用户是否存在 基本语法 id 用户名 案例实操 查看用户是否存在 12[root@hadoop100 bin]# id zhangsanuid=1001(zhangsan) gid=1001(zhangsan) groups=1001(zhangsan) cat /etc/passwd 查看创建了哪些用户1[root@hadoop100 ~]# cat /etc/passwd su 切换用户su: swith user 切换用户 基本语法 su 用户名称 （功能描述：切换用户，只能获得用户的执行权限，不能获得环境变量） su - 用户名称 （功能描述：切换到用户并获得该用户的环境变量及执行权限） 案例实操 切换用户 1234567[root@hadoop100 ~]# su zhangsan[zhangsan@hadoop100 root]$ echo $PATH/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin:/root/bin[root@hadoop100 ~]# su - zhangsan[zhangsan@hadoop100 ~]$ echo $PATH/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/zhangsan/.local/bin:/home/zhangsan/bin userdel 删除用户 基本语法 userdel 用户名 （功能描述：删除用户但保存用户主目录） userdel -r 用户名 （功能描述：用户和用户主目录，都删除） 选项说明 选项 功能 -r 删除用户的同时，删除与用户相关的所有文件。 案例实操 删除用户但保存用户主目录 123[root@hadoop100 ~]# userdel zhangsan[root@hadoop100 ~]# ll /home/drwx------. 5 1001 1001 128 May 7 20:42 zhangsan 删除用户和用户主目录，都删除 123456[root@hadoop100 ~]# useradd zhangsanuseradd: warning: the home directory already exists.Not copying any file from skel directory into it.[root@hadoop100 ~]# userdel -r zhangsan[root@hadoop100 ~]# ll /home/ who 查看登录用户信息 基本语法 whoami （功能描述：显示自身用户名称） who am i （功能描述：显示登录用户的用户名） 案例实操 显示自身用户名称 12[root@hadoop100 ~]# whoamiroot 显示登录用户的用户名 1234[root@hadoop100 ~]# su - curiophilLast login: Fri May 7 20:22:18 CST 2021 on pts/0[curiophil@hadoop100 ~]$ who am i root pts/0 2021-05-07 20:42 (192.168.1.3) sudo 设置普通用户具有root权限 添加zhangsan用户，并对其设置密码 修改配置文件 1[root@hadoop100 ~]#vi /etc/sudoers 修改 /etc/sudoers 文件，找到下面一行(99行)，在root下面添加一行，如下所示： 123##Allow root to run any commands anywhereroot ALL&#x3D;(ALL) ALLzhangsan ALL&#x3D;(ALL) ALL 或者配置成采用sudo命令时，不需要输入密码 123## Allow root to run any commands anywhereroot ALL=(ALL) ALLzhangsan ALL=(ALL) NOPASSWD:ALL 修改完毕，现在可以用zhangsan帐号登录，然后用命令 sudo ，即可获得root权限进行操作。 案例实操 用普通用户在/opt目录下创建一个文件夹 12[zhangsan@hadoop100 opt]$ sudo mkdir module[root@hadoop100 opt]# chown zhangsan:zhangsan module/ usermod 修改用户 基本语法 usermod -g 用户组 用户名 选项说明 选项 功能 -g 修改用户的初始登录组，给定的组必须存在 案例实操 将用户加入到用户组 1[root@hadoop100 opt]#usermod -g root zhangsan 用户组管理命令每个用户都有一个用户组，系统可以对一个用户组中的所有用户进行集中管理。不同Linux 系统对用户组的规定有所不同， 如Linux下的用户属于与它同名的用户组，这个用户组在创建用户时同时创建。 用户组的管理涉及用户组的添加、删除和修改。组的增加、删除和修改实际上就是对/etc/group文件的更新。 groupadd 新增组1[root@hadoop100 ~]#groupadd lisi groupdel 删除组1[root@hadoop100 ~]# groupdel lisi groupmod 修改组 基本语法 groupmod -n 新组名 老组名 选项说明 选项 功能描述 -n&lt;新组名&gt; 指定工作组的新组名 案例实操 修改lisi组名称为lisi1 12[root@hadoop100 ~]#groupadd lisi[root@hadoop100 ~]# groupmod -n lisi1 lisi cat /etc/group 查看创建了哪些组1[root@hadoop100 ~]# cat /etc/group 文件权限类文件属性Linux系统是一种典型的多用户系统，不同的用户处于不同的地位，拥有不同的权限。为了保护系统的安全性，Linux系统对不同的用户访问同一文件（包括目录文件）的权限做了不同的规定。在Linux中我们可以使用ll或者ls -l命令来显示一个文件的属性以及文件所属的用户和组。 从左到右的10个字符表示，如图所示。 如果没有权限，就会出现减号[ - ]而已。从左至右用0-9这些数字来表示: 0首位表示类型 在Linux中第一个字符代表这个文件是目录、文件或链接文件等等 - 代表文件 d 代表目录 l 链接文档(link file)； 第1-3位确定属主（该文件的所有者）拥有该文件的权限。—-User 第4-6位确定属组（所有者的同组用户）拥有该文件的权限，—-Group 第7-9位确定其他用户拥有该文件的权限 —-Other rxw作用文件和目录的不同解释 作用到文件 [ r ]代表可读(read): 可以读取，查看 [ w ]代表可写(write): 可以修改，但是不代表可以删除该文件，删除一个文件的前提条件是对该文件所在的目录有写权限，才能删除该文件. [ x ]代表可执行(execute):可以被系统执行 作用到目录 [ r ]代表可读(read): 可以读取，ls查看目录内容 [ w ]代表可写(write): 可以修改，目录内创建+删除+重命名目录 [ x ]代表可执行(execute):可以进入该目录 案例实操 123[root@hadoop100 ~]# lltotal 8-rw-------. 1 root root 1533 Apr 29 17:24 anaconda-ks.cfg 如果查看到是文件：链接数指的是硬链接个数。创建硬链接方法 ln [原文件] [目标文件] 1[root@hadoop101 ~]# ln a/b/a.txt b.txt 如果查看的是文件夹：链接数指的是子文件夹个数。 chmod 改变权限 基本语法 第一种方式变更权限 ​ chmod [{ugoa}{+-=}{rwx}] 文件或目录 第二种方式变更权限 ​ chmod [mode=421 ] [文件或目录] 经验技巧 u:所有者 g:所有组 o:其他人 a:所有人(u、g、o的总和) r=4 w=2 x=1 rwx=4+2+1=7 案例实操 修改文件使其所属主用户具有执行权限 12[root@hadoop100 ~]# touch a.txt[root@hadoop100 ~]# chmod u+x a.txt 修改文件使其所属组用户具有执行权限 1[root@hadoop100 ~]# chmod g+x a.txt 修改文件所属主用户执行权限,并使其他用户具有执行权限 1[root@hadoop100 ~]# chmod u-x,o+x a.txt 采用数字的方式，设置文件所有者、所属组、其他用户都具有可读可写可执行权限。 1[root@hadoop100 ~]# chmod 777 a.txt 修改整个文件夹里面的所有文件的所有者、所属组、其他用户都具有可读可写可执行权限。 1[root@hadoop100 ~]# chmod -R 777 a/ chown 改变所有者 基本语法 chown [选项] [最终用户] [文件或目录] （功能描述：改变文件或者目录的所有者） 选项说明 选项 功能 -R 递归操作 案例实操 修改文件所有者 12[root@hadoop100 ~]# chown curiophil a.txt -rwxr-xr--. 1 curiophil root 0 May 7 21:24 a.txt 递归改变文件所有者和所有组 12[root@hadoop100 ~]# chown -R curiophil:curiophil a/drwxr-xr-x. 3 curiophil curiophil 15 May 7 21:32 a chgrp 改变所属组 基本语法 chgrp [最终用户组] [文件或目录] （功能描述：改变文件或者目录的所属组） 案例实操 修改文件的所属组 123[root@hadoop100 ~]# chgrp curiophil a.txt [root@hadoop100 ~]# ll-rwxr-xr--. 1 curiophil curiophil 0 May 7 21:35 a.txt 搜索查找类find 查找文件或者目录find指令将从指定目录向下递归地遍历其各个子目录，将满足条件的文件显示在终端。 基本语法 find [搜索范围] [选项] 选项说明 选项 功能 -name&lt;查询方式&gt; 按照指定的文件名查找模式查找文件 -user&lt;用户名&gt; 查找属于指定用户名所有文件 -size&lt;文件大小&gt; 按照指定的文件大小查找文件。 案例实操 按文件名：根据名称查找/目录下的filename.txt文件。 1[root@hadoop100 ~]# find -name &quot;*.txt&quot; 按拥有者：查找/opt目录下，用户名称为-user的文件 1[root@hadoop100 ~]# find /opt/ -user root 按文件大小：在/home目录下查找大于200m的文件（+n 大于 -n小于 n等于） 1[root@hadoop100 ~]# find /home/ -size +204800 grep 过滤查找及“|”管道符管道符，“|”，表示将前一个命令的处理结果输出传递给后面的命令处理 基本语法 grep 选项 查找内容 源文件 选项说明 -n 显示匹配行及行号 案例实操 查找某文件在第几行 1[root@hadoop100 ~]# ls | grep -n Doc which 查找命令查找命令在那个目录下 基本语法 which 命令 案例实操 123[root@hadoop100 ~]# which llalias ll=&#x27;ls -l --color=auto&#x27; /usr/bin/ls 压缩和解压类gzip/gunzip 压缩 基本语法 gzip 文件 （功能描述：压缩文件，只能将文件压缩为*.gz文件） gunzip 文件.gz （功能描述：解压缩文件命令） 经验技巧 （1）只能压缩文件不能压缩目录 （2）不保留原来的文件 案例实操 gzip压缩 1[root@hadoop100 ~]# gzip a.txt gunzip解压缩文件 1[root@hadoop100 ~]# gunzip a.txt.gz zip/unzip 压缩 基本语法 zip [选项] XXX.zip 将要压缩的内容 （功能描述：压缩文件和目录的命令） unzip [选项] XXX.zip （功能描述：解压缩文件） 选项说明 zip 选项 功能 -r 压缩目录 unzip 选项 功能 -d&lt;目录&gt; 指定解压后文件的存放目录 经验技巧 zip 压缩命令在window/linux都通用，可以压缩目录且保留源文件。 案例实操 压缩 a.txt 和b.txt，压缩后的名称为mypackage.zip 123[root@hadoop100 ~]# zip mypackage.zip a.txt b.txt adding: a.txt (stored 0%) adding: b.txt (stored 0%) 解压 mypackage.zip 1[root@hadoop100 ~]# unzip mypackage.zip 解压mypackage.zip到指定目录-d 1234[root@hadoop100 ~]# unzip -d mypackage mypackage.zip Archive: mypackage.zip extracting: mypackage/a.txt extracting: mypackage/b.txt tar 打包 基本语法 tar [选项] XXX.tar.gz 将要打包进去的内容 （功能描述：打包目录，压缩后的文件格式.tar.gz） 选项说明 选项 功能 -z 打包同时压缩 -c 产生.tar打包文件 -v 显示详细信息 -f 指定压缩后的文件名 -x 解包.tar文件 案例实操 压缩多个文件 1[root@hadoop100 ~]# tar -zcvf mypackage.tar.gz a.txt b.txt 压缩目录 1[root@hadoop100 ~]# tar -zcvf a.tar.gz a/ 解压到当前目录 1[root@hadoop100 ~]# tar -zxvf a.tar.gz 解压到指定目录 12[root@hadoop100 ~]# mkdir a[root@hadoop100 ~]# tar -zxvf mypackage.tar.gz -C a/ 磁盘分区类df 查看磁盘空间使用情况df: disk free 空余硬盘 基本语法 df 选项 （功能描述：列出文件系统的整体磁盘使用量，检查文件系统的磁盘空间占用情况） 选项说明 -h 以人们较易阅读的 GBytes, MBytes, KBytes 等格式自行显示 案例实操 查看磁盘使用情况 12345678910[root@hadoop100 a]# df -hFilesystem Size Used Avail Use% Mounted ondevtmpfs 974M 0 974M 0% /devtmpfs 991M 0 991M 0% /dev/shmtmpfs 991M 11M 980M 2% /runtmpfs 991M 0 991M 0% /sys/fs/cgroup/dev/mapper/centos-root 47G 4.9G 43G 11% //dev/sda1 1014M 172M 843M 17% /boottmpfs 199M 12K 199M 1% /run/user/42tmpfs 199M 0 199M 0% /run/user/0 fdisk 查看分区 基本语法 fdisk -l （功能描述：查看磁盘分区详情） 选项说明 -l 显示所有硬盘的分区列表 经验技巧 该命令必须在root用户下才能使用 功能说明 Device：分区序列 Boot：引导 Start：从X磁柱开始 End：到Y磁柱结束 Blocks：容量 Id：分区类型ID System：分区类型 案例实操 查看系统分区情况 1234567891011121314151617181920212223[root@hadoop100 a]# fdisk -lDisk /dev/sda: 53.7 GB, 53687091200 bytes, 104857600 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x000a0d1e Device Boot Start End Blocks Id System/dev/sda1 * 2048 2099199 1048576 83 Linux/dev/sda2 2099200 104857599 51379200 8e Linux LVMDisk /dev/mapper/centos-root: 50.5 GB, 50457477120 bytes, 98549760 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk /dev/mapper/centos-swap: 2147 MB, 2147483648 bytes, 4194304 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytes mount/umount 挂载/卸载对于Linux用户来讲，不论有几个分区，分别分给哪一个目录使用，它总归就是一个根目录、一个独立且唯一的文件结构。 Linux中每个分区都是用来组成整个文件系统的一部分，它在用一种叫做“挂载”的处理方法，它整个文件系统中包含了一整套的文件和目录，并将一个分区和一个目录联系起来，要载入的那个分区将使它的存储空间在这个目录下获得。 基本语法 mount [-t vfstype] [-o options] device dir （功能描述：挂载设备） umount 设备文件名或挂载点 （功能描述：卸载设备） 参数说明 参数 功能 -t vfstype 指定文件系统的类型，通常不必指定。mount 会自动选择正确的类型。常用类型有：光盘或光盘镜像：iso9660，DOS fat16文件系统：msdos，Windows 9x fat32文件系统：vfat， Windows NT ntfs文件系统：ntfs， Mount Windows文件网络共享：smbfs， UNIX(LINUX) 文件网络共享：nfs -o options 主要用来描述设备或档案的挂接方式。常用的参数有：loop：用来把一个文件当成硬盘分区挂接上系统， ro：采用只读方式挂接设备， rw：采用读写方式挂接设备， iocharset：指定访问文件系统所用字符集 device 要挂接(mount)的设备 dir 设备在系统上的挂接点(mount point) 案例实操 挂载光盘镜像文件 1[root@hadoop100 mnt]# mount /dev/cdrom /mnt/cd/ 卸载光盘镜像文件 123[root@hadoop100 mnt]# umount /mnt/cd[root@hadoop100 mnt]# ll cd total 0 设置开机自动挂载 1234[root@hadoop100 mnt]# vim /etc/fstab# 尾部加一行/dev/cdrom /mnt/cd 进程线程类进程是正在执行的一个程序或命令，每一个进程都是一个运行的实体，都有自己的地址空间，并占用一定的系统资源。 ps 查看当前系统进程状态ps:process status 进程状态 基本语法 ps -aux | grep xxx （功能描述：查看系统中所有进程） ps -ef | grep xxx （功能描述：可以查看子父进程之间的关系） 选项说明 选项 功能 -a 选择所有进程 -u 显示所有用户的所有进程 -x 显示没有终端的进程 功能说明 （1）ps -aux显示信息说明 USER：该进程是由哪个用户产生的 PID：进程的ID号 %CPU：该进程占用CPU资源的百分比，占用越高，进程越耗费资源； %MEM：该进程占用物理内存的百分比，占用越高，进程越耗费资源； VSZ：该进程占用虚拟内存的大小，单位KB； RSS：该进程占用实际物理内存的大小，单位KB； TTY：该进程是在哪个终端中运行的。其中tty1-tty7代表本地控制台终端，tty1-tty6是本地的字符界面终端，tty7是图形终端。pts/0-255代表虚拟终端。 STAT：进程状态。常见的状态有：R：运行、S：睡眠、T：停止状态、s：包含子进程、+：位于后台 START：该进程的启动时间 TIME：该进程占用CPU的运算时间，注意不是系统时间 COMMAND：产生此进程的命令名 （2）ps -ef显示信息说明 UID：用户ID PID：进程ID PPID：父进程ID C：CPU用于计算执行优先级的因子。数值越大，表明进程是CPU密集型运算，执行优先级会降低；数值越小，表明进程是I/O密集型运算，执行优先级会提高 STIME：进程启动的时间 TTY：完整的终端名称 TIME：CPU时间 CMD：启动进程所用的命令和参数 经验技巧 如果想查看进程的CPU占用率和内存占用率，可以使用aux; 如果想查看进程的父进程ID可以使用ef; 案例实操 12345678910[root@hadoop100 ~]# ps -auxUSER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 1 0.3 0.3 193816 6372 ? Ss 12:00 0:02 /usr/lib/systemd/systemd --switched-root --system --deserialize 22root 2 0.0 0.0 0 0 ? S 12:00 0:00 [kthreadd]root 3 0.0 0.0 0 0 ? S 12:00 0:00 [kworker/0:0]root 4 0.0 0.0 0 0 ? S&lt; 12:00 0:00 [kworker/0:0H]root 6 0.0 0.0 0 0 ? S 12:00 0:00 [ksoftirqd/0]root 7 0.6 0.0 0 0 ? S 12:00 0:04 [migration/0]root 8 0.0 0.0 0 0 ? S 12:00 0:00 [rcu_bh]root 9 0.0 0.0 0 0 ? S 12:00 0:00 [rcu_sched] 123456789101112[root@hadoop100 ~]# ps -efUID PID PPID C STIME TTY TIME CMDroot 1 0 0 12:00 ? 00:00:02 /usr/lib/systemd/systemd --switched-root --system --deserialize 22root 2 0 0 12:00 ? 00:00:00 [kthreadd]root 3 2 0 12:00 ? 00:00:00 [kworker/0:0]root 4 2 0 12:00 ? 00:00:00 [kworker/0:0H]root 6 2 0 12:00 ? 00:00:00 [ksoftirqd/0]root 7 2 0 12:00 ? 00:00:04 [migration/0]root 8 2 0 12:00 ? 00:00:00 [rcu_bh]root 9 2 0 12:00 ? 00:00:00 [rcu_sched]root 10 2 0 12:00 ? 00:00:00 [lru-add-drain]root 11 2 0 12:00 ? 00:00:00 [watchdog/0] kill 终止进程 基本语法 kill [选项] 进程号 （功能描述：通过进程号杀死进程） killall 进程名称 （功能描述：通过进程名称杀死进程，也支持通配符，这在系统因负载过大而变得很慢时很有用） 选项说明 -9 表示强迫进程立即停止 案例实操 杀死浏览器进程 1[root@hadoop100 ~]# kill -9 3077 通过进程名称杀死进程 1[root@hadoop100 ~]# killall firefox pstree 查看进程树 基本语法 pstree [选项] 选项说明 选项 功能 -p 显示进程的PID -u 显示进程的所属用户 案例实操 显示进程pid 1[root@hadoop100 ~]# pstree -p 显示进程所属用户 1[root@hadoop100 ~]# pstree -u top 查看系统健康状态 基本命令 top [选项] 选项说明 选项 功能 -d 秒数 指定top命令每隔几秒更新。默认是3秒在top命令的交互模式当中可以执行的命令： -i 使top不显示任何闲置或者僵死进程 -p 通过指定监控进程ID来仅仅监控某个进程的状态。 操作说明 操作 功能 P 以CPU使用率排序，默认就是此项 M 以内存的使用率排序 N 以PID排序 q 退出top 查询结果字段解释 第一行信息为任务队列信息 内容 说明 12:26:46 系统当前时间 up 1 day, 13:32 系统的运行时间，本机已经运行1天 13小时32分钟 2 users 当前登录了两个用户 load average: 0.00, 0.00, 0.00 系统在之前1分钟，5分钟，15分钟的平均负载。一般认为小于1时，负载较小。如果大于1，系统已经超出负荷。 第二行为进程信息 Tasks: 95 total 系统中的进程总数 1 running 正在运行的进程数 94 sleeping 睡眠的进程 0 stopped 正在停止的进程 0 zombie 僵尸进程。如果不是0，需要手工检查僵尸进程 第三行为CPU信息 Cpu(s): 0.1%us 用户模式占用的CPU百分比 0.1%sy 系统模式占用的CPU百分比 0.0%ni 改变过优先级的用户进程占用的CPU百分比 99.7%id 空闲CPU的CPU百分比 0.1%wa 等待输入/输出的进程的占用CPU百分比 0.0%hi 硬中断请求服务占用的CPU百分比 0.1%si 软中断请求服务占用的CPU百分比 0.0%st st（Steal time）虚拟时间百分比。就是当有虚拟机时，虚拟CPU等待实际CPU的时间百分比。 第四行为物理内存信息 Mem: 625344k total 物理内存的总量，单位KB 571504k used 已经使用的物理内存数量 53840k free 空闲的物理内存数量，我们使用的是虚拟机，总共只分配了628MB内存，所以只有53MB的空闲内存了 65800k buffers 作为缓冲的内存数量 第五行为交换分区（swap）信息 Swap: 524280k total 交换分区（虚拟内存）的总大小 0k used 已经使用的交互分区的大小 524280k free 空闲交换分区的大小 409280k cached 作为缓存的交互分区的大小 案例实操 123[root@hadoop100 ~]# top -d 1[root@hadoop100 ~]# top -i[root@hadoop100 ~]# top -p 3306 执行上述命令后，可以按P、M、N对查询出的进程结果进行排序。 netstat 显示网络统计信息和端口占用情况 基本语法 netstat -anp |grep 进程号 （功能描述：查看该进程网络信息） netstat -nlp | grep 端口号 （功能描述：查看网络端口号占用情况） 选项说明 选项 功能 -n 拒绝显示别名，能显示数字的全部转化成数字 -l 仅列出有在listen（监听）的服务状态 -p 表示显示哪个进程在调用 案例实操 通过进程号查看该进程的网络信息 1[root@hadoop100 ~]# netstat -anp | grep 4120 查看某端口号是否被占用 1[root@hadoop100 ~]# netstat -nlp | grep 4120 crond 系统定时任务crond 服务管理 重新启动crond服务 1[root@hadoop100 ~]# service crond restart crontab 定时任务设置 基本语法 crontab [选项] 选项说明 选项 功能 -e 编辑crontab定时任务 -l 查询crontab任务 -r 删除当前用户所有的crontab任务 参数说明 1[root@hadoop100 ~]# crontab -e （1）进入crontab编辑界面。会打开vim编辑你的工作。 * 执行的任务 项目 含义 范围 第一个“*” 一小时当中的第几分钟 0-59 第二个“*” 一天当中的第几小时 0-23 第三个“*” 一个月当中的第几天 1-31 第四个“*” 一年当中的第几月 1-12 第五个“*” 一周当中的星期几 0-7（0和7都代表星期日） （2）特殊符号 特殊符号 含义 * 代表任何时间。比如第一个“*”就代表一小时中每分钟都执行一次的意思。 ， 代表不连续的时间。比如“0 8,12,16 * 命令”，就代表在每天的8点0分，12点0分，16点0分都执行一次命令 - 代表连续的时间范围。比如“0 5 1-6命令”，代表在周一到周六的凌晨5点0分执行命令 */n 代表每隔多久执行一次。比如“/10 * 命令”，代表每隔10分钟就执行一遍命令 （3）特定时间执行命令 时间 含义 45 22 * 命令 在22点45分执行命令 0 17 1 命令 每周1 的17点0分执行命令 0 5 1,15 命令 每月1号和15号的凌晨5点0分执行命令 40 4 1-5 命令 每周一到周五的凌晨4点40分执行命令 /10 4 命令 每天的凌晨4点，每隔10分钟执行一次命令 0 0 1,15 * 1 命令 每月1号和15号，每周1的0点0分都会执行命令。注意：星期几和几号最好不要同时出现，因为他们定义的都是天。非常容易让管理员混乱。 案例实操 每隔1分钟，向/root/a.txt文件中添加一个“hello world!” 1*/1 * * * * /bin/echo ”hello world!” &gt;&gt; /root/a.txt","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"}]},{"title":"linux服务管理(centos7)","slug":"linux服务管理","date":"2021-04-29T14:39:41.000Z","updated":"2021-10-12T08:24:32.657Z","comments":true,"path":"2021/04/29/linux服务管理/","link":"","permalink":"http://example.com/2021/04/29/linux%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86/","excerpt":"基本语法（centos7）临时后台服务管理开启服务1systemctl start 服务名 关闭服务1systemctl stop 服务名 重新启动服务1systemctl restart 服务名","text":"基本语法（centos7）临时后台服务管理开启服务1systemctl start 服务名 关闭服务1systemctl stop 服务名 重新启动服务1systemctl restart 服务名 查看服务状态1systemctl status 服务名 查看正在运行的服务1systemctl --type service 设置后台服务的自启配置查看所有服务器自启配置1systemctl list-unit-files 关掉指定服务的自动启动1systemctl disable 服务名 开启指定服务的自动启动1systemctl enable 服务名 查看服务开机启动状态1systemctl is-enabled","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"centos7","slug":"centos7","permalink":"http://example.com/tags/centos7/"}]},{"title":"Linux目录结构","slug":"Linux目录结构","date":"2021-04-29T10:25:15.000Z","updated":"2021-10-12T08:22:35.535Z","comments":true,"path":"2021/04/29/Linux目录结构/","link":"","permalink":"http://example.com/2021/04/29/Linux%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84/","excerpt":"/bin (/usr/bin 、 /usr/local/bin) 是Binary的缩写, 这个目录存放着最经常使用的命令 /sbin (/usr/sbin 、 /usr/local/sbin) s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序 /home 存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的 /root 该目录为系统管理员，也称作超级权限者的用户主目录 /lib 系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。","text":"/bin (/usr/bin 、 /usr/local/bin) 是Binary的缩写, 这个目录存放着最经常使用的命令 /sbin (/usr/sbin 、 /usr/local/sbin) s就是Super User的意思，这里存放的是系统管理员使用的系统管理程序 /home 存放普通用户的主目录，在Linux中每个用户都有一个自己的目录，一般该目录名是以用户的账号命名的 /root 该目录为系统管理员，也称作超级权限者的用户主目录 /lib 系统开机所需要最基本的动态连接共享库，其作用类似于Windows里的DLL文件。几乎所有的应用程序都需要用到这些共享库。 /lost+found 这个目录一般情况下是空的，当系统非法关机后，这里就存放了一些文件。 /etc 所有的系统管理所需要的配置文件和子目录 /usr 这是一个非常重要的目录，用户的很多应用程序和文件都放在这个目录下，类似与windows下的program files目录 /boot 这里存放的是启动Linux时使用的一些核心文件，包括一些连接文件以及镜像文件，自己的安装别放这里 /proc 这个目录是一个虚拟的目录，它是系统内存的映射，我们可以通过直接访问这个目录来获取系统信息 /srv service缩写，该目录存放一些服务启动之后需要提取的数据 /sys 这是linux2.6内核的一个很大的变化。该目录下安装了2.6内核中新出现的一个文件系统 sysfs /tmp 这个目录是用来存放一些临时文件的 /dev 类似于windows的设备管理器，把所有的硬件用文件的形式存储 /media linux系统会自动识别一些设备，例如U盘、光驱等等，当识别后，linux会把识别的设备挂载到这个目录下 /mnt 系统提供该目录是为了让用户临时挂载别的文件系统的，我们可以将外部的存储挂载在/mnt/上，然后进入该目录就可以查看里的内容了 /opt 这是给主机额外安装软件所摆放的目录。比如你安装一个ORACLE数据库则就可以放到这个目录下。默认是空的。 /usr/local 这是另一个给主机额外安装软件所摆放的目录。一般是通过编译源码方式安装的程序 /var 这个目录中存放着在不断扩充着的东西，我们习惯将那些经常被修改的目录放在这个目录下。包括各种日志文件 /selinux SELinux是一种安全子系统,它能控制程序只能访问特定文件。","categories":[],"tags":[{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"centos7","slug":"centos7","permalink":"http://example.com/tags/centos7/"}]},{"title":"JupyterNotebook中使用conda虚拟环境","slug":"JupyterNotebook中使用conda虚拟环境","date":"2021-03-21T05:24:15.000Z","updated":"2021-05-26T10:30:33.370Z","comments":true,"path":"2021/03/21/JupyterNotebook中使用conda虚拟环境/","link":"","permalink":"http://example.com/2021/03/21/JupyterNotebook%E4%B8%AD%E4%BD%BF%E7%94%A8conda%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/","excerpt":"","text":"root环境安装nb_conda1conda install nb_conda 虚拟环境安装ipykernel1conda install ipykernel 参考链接 Anaconda 和 Jupyter notebook","categories":[],"tags":[{"name":"python环境","slug":"python环境","permalink":"http://example.com/tags/python%E7%8E%AF%E5%A2%83/"}]},{"title":"GA,SA,GRNN","slug":"GA-SA-GRNN","date":"2021-03-20T06:53:38.000Z","updated":"2021-03-20T07:01:09.710Z","comments":true,"path":"2021/03/20/GA-SA-GRNN/","link":"","permalink":"http://example.com/2021/03/20/GA-SA-GRNN/","excerpt":"","text":"参考链接 模拟退火算法 遗传算法 广义回归神经网路","categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"重装系统","slug":"重装系统","date":"2021-03-06T14:17:29.000Z","updated":"2021-10-12T08:20:27.389Z","comments":true,"path":"2021/03/06/重装系统/","link":"","permalink":"http://example.com/2021/03/06/%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"参考链接 手把手教你如何重装win10系统，自己动手安装系统其实很简单 win10 激活软件链接：https://pan.baidu.com/s/1ptVXNrSte5A3-Lt1EjhbaA提取码：pebm","categories":[],"tags":[{"name":"系统","slug":"系统","permalink":"http://example.com/tags/%E7%B3%BB%E7%BB%9F/"}]},{"title":"latex的常用数学符号","slug":"latex数学符号","date":"2021-03-02T12:51:04.000Z","updated":"2021-10-12T08:20:00.012Z","comments":true,"path":"2021/03/02/latex数学符号/","link":"","permalink":"http://example.com/2021/03/02/latex%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7/","excerpt":"","text":"参考链接 Latex所有常用数学符号整理","categories":[],"tags":[{"name":"latex","slug":"latex","permalink":"http://example.com/tags/latex/"}]},{"title":"next主题使用Latex公式","slug":"next主题使用公式","date":"2021-03-01T13:06:22.000Z","updated":"2021-11-02T06:01:39.058Z","comments":true,"path":"2021/03/01/next主题使用公式/","link":"","permalink":"http://example.com/2021/03/01/next%E4%B8%BB%E9%A2%98%E4%BD%BF%E7%94%A8%E5%85%AC%E5%BC%8F/","excerpt":"","text":"Install Dependencies123# 需要先卸载默认的渲染引擎npm un hexo-renderer-marked --savenpm i hexo-renderer-kramed --save 12345678910111213141516# 这是博客目录下的本地安装的包D:\\blog&gt;npm ls --depth=0hexo-site@0.0.0 D:\\blog+-- hexo@5.4.0+-- hexo-deployer-git@3.0.0+-- hexo-generator-archive@1.0.0+-- hexo-generator-category@1.0.0+-- hexo-generator-index@2.0.0+-- hexo-generator-json-content@4.2.3+-- hexo-generator-tag@1.0.0+-- hexo-renderer-ejs@1.0.0+-- hexo-renderer-kramed@0.1.4+-- hexo-renderer-pug@1.0.0+-- hexo-renderer-stylus@2.0.1+-- hexo-server@2.0.0`-- hexo-theme-landscape@0.0.3 Configurenext文件夹下_config.yml这两个地方默认为False，更改为True 1234mathjax: enable: true # See: https://mhchem.github.io/MathJax-mhchem/ mhchem: true 使用在每个md文件的头部加上mathjax: true即可 测试公式$\\sigma(x)=\\frac{1}{1+e^{-x}}$ 参考链接 Hexo+Next: 使用Latex公式 hexo个人博客next主题使用LaTeX公式","categories":[],"tags":[{"name":"latex","slug":"latex","permalink":"http://example.com/tags/latex/"},{"name":"next","slug":"next","permalink":"http://example.com/tags/next/"}]}],"categories":[],"tags":[{"name":"bigdata","slug":"bigdata","permalink":"http://example.com/tags/bigdata/"},{"name":"面试","slug":"面试","permalink":"http://example.com/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Hadoop","slug":"Hadoop","permalink":"http://example.com/tags/Hadoop/"},{"name":"Hive","slug":"Hive","permalink":"http://example.com/tags/Hive/"},{"name":"Kafka","slug":"Kafka","permalink":"http://example.com/tags/Kafka/"},{"name":"HBase","slug":"HBase","permalink":"http://example.com/tags/HBase/"},{"name":"Spark","slug":"Spark","permalink":"http://example.com/tags/Spark/"},{"name":"踩坑","slug":"踩坑","permalink":"http://example.com/tags/%E8%B8%A9%E5%9D%91/"},{"name":"linux","slug":"linux","permalink":"http://example.com/tags/linux/"},{"name":"shell","slug":"shell","permalink":"http://example.com/tags/shell/"},{"name":"mysql","slug":"mysql","permalink":"http://example.com/tags/mysql/"},{"name":"centos7","slug":"centos7","permalink":"http://example.com/tags/centos7/"},{"name":"python环境","slug":"python环境","permalink":"http://example.com/tags/python%E7%8E%AF%E5%A2%83/"},{"name":"算法","slug":"算法","permalink":"http://example.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"系统","slug":"系统","permalink":"http://example.com/tags/%E7%B3%BB%E7%BB%9F/"},{"name":"latex","slug":"latex","permalink":"http://example.com/tags/latex/"},{"name":"next","slug":"next","permalink":"http://example.com/tags/next/"}]}
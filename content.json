{"meta":{"title":"珍品菲尔的博客","subtitle":"Lifetime Learner","description":"整天摸鱼不上进的研究生","author":"Curio Phil","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"第一次使用","slug":"第一次使用","date":"2021-02-22T07:07:27.000Z","updated":"2021-02-22T07:17:40.247Z","comments":true,"path":"2021/02/22/第一次使用/","link":"","permalink":"http://example.com/2021/02/22/%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%BD%BF%E7%94%A8/","excerpt":"","text":"123456第1个epoch train, loss: 112.97454833984375, accuracy: 0.6702225804328918第1个epoch validation, loss: 39.812904357910156, accuracy: 0.6370246410369873第2个epoch train, loss: 111.52589416503906, accuracy: 0.6781042814254761第2个epoch validation, loss: 39.61885070800781, accuracy: 0.6407220959663391第3个epoch train, loss: 110.03665924072266, accuracy: 0.6859859824180603第3个epoch validation, loss: 39.905391693115234, accuracy: 0.6394481658935547","categories":[],"tags":[{"name":"实验结果","slug":"实验结果","permalink":"http://example.com/tags/%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C/"}]},{"title":"数据处理","slug":"数据处理","date":"2021-02-15T13:06:22.000Z","updated":"2021-03-01T10:50:24.024Z","comments":true,"path":"2021/02/15/数据处理/","link":"","permalink":"http://example.com/2021/02/15/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/","excerpt":"","text":"$preprocessed01.py$ 用于将评论的标签添加到评论上面，并且保存其中的全部虚假评论以及和虚假评论等数量的真实评论，以构造一个平衡的数据集，其真实评论数量和虚假评论数量一致，1为真实评论，0为虚假评论 $preprocessed01.txt$ 是上一步所得到得文件，共有5个字段，$user_id$, $prod_id$, $date$, $review_text$, $label$ $preprocessed02.py$ 打乱顺序数据集，将句子单词小写，分词 $preprocessed02.txt$ 添加了字段 $tokenized_review_text$ ，此字段为使用 $nltk.word_tokenize$ 得到的分词后的列表，注意：list对象存到txt文件里变成了字符串，所以此txt文件没用 $preprocessed02.pkl$ 保存 $DataFrame$ 对象，为全部虚假评论以及和虚假评论等数量的真实评论，共6个字段，新加了 $tokenized_review_text$ $preprocessed03.py$ 训练 $Word2Vec$ ，得到100维的词向量表示，保存模型为$w2v$，加载时使用$Word2Vec.load(filename)$函数，然后将$DataFrame$按照$3:1:1$划分，得到以下三个文件，共6个字段 $preprocessed03train.pkl$ 是$60%$的数据集，用于训练，$preprocessed03validation.pkl$ 是$20%$的数据集，用于验证$preprocessed03test.pkl$ 是$20%$的数据集，用于测试 $w2v.model$ 为保存的训练好的词向量 使用$nltk.pos_tag$做词性标注，添加了字段$pos_val$列，得到以下文件，删除了$review_text$列，共6个字段，得到以下文件 $preprocessed03.pkl$ $user_id$, $prod_id$, $date$, $label$ ，$tokenized_review_text$，$pos_val$ $preprocessed03.1train.pkl$ 是$60%$的数据集，用于训练，$preprocessed03.1validation.pkl$ 是$20%$的数据集，用于验证$preprocessed03.1test.pkl$ 是$20%$的数据集，用于测试 $preprocessed04.py$ 计算了每个词的$tf-idf$值，使用的是sklearn的CountVectorizer()及TfidfTransformer()，将所有值保存到了$preprocessed04.pkl$中 按照3：1：1划分了训练集，验证集，测试集对应3个文件 $preprocessed05.py$计算了每条评论的情感分析，字段叫做$sentiment_analysis$，情感分析保存为一个元组,值有两个，一个是情感极性，一个是主观性 $preprocessed06.py$计算了每条评论的LIWC值，字段记作$liwc$，保存为长度为64的list $preprocessed07.py$重新用BERT做分词，由于分词的变化，重新计算了以上那些词性特征，tf-idf，情感分析特征，liwc特征，分为以下字段，由于BERT相当消耗硬件资源，笔记本跑不动 0 1 2 3 4 5 6 7 8 9 10 user_id prod_id date label rating tokenized_review_text pos_val tf_idf sentiment_analysis liwc review_text $preprocessed08.py$","categories":[],"tags":[{"name":"数据处理的readme","slug":"数据处理的readme","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84readme/"}]}],"categories":[],"tags":[{"name":"实验结果","slug":"实验结果","permalink":"http://example.com/tags/%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C/"},{"name":"数据处理的readme","slug":"数据处理的readme","permalink":"http://example.com/tags/%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84readme/"}]}